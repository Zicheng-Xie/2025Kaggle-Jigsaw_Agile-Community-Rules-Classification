{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Speed Run: 10-Minute Triplet Pretraining and FAISS Clustering\n","\n","## The Idea\n","The core idea and challenge I set for myself is to pretrain embeddings on test data while achieving acceptable scores as quickly as possible. This includes domain adaptation of competition & test data to pretrained embeddings by using pretraining techniques, then comparing the given body to positive and negative examples to see if it's more violating than compliant. Since the metric is AUC, we can leverage distance metrics rather than using probabilities since only the ranking matters.\n","\n","## Steps\n","\n","### 1. **Data Preparation**\n","- Clean text\n","- Create triplets from test data using rules and examples: `(rule, compliant_comment, violating_comment)` (This might be a little bit confusing since it's the reverse of the competition logic. But when the rules are the anchor, it makes more sense to create triplets like this for pretraining.)\n","- Subsample to 10% for efficient training\n","\n","### 2. **Pretrain**\n","- Base: BGE-small-en-v1.5 embeddings\n","- Loss: Triplet loss (margin=0.25)\n","- Quick training: 1 epoch, batch size 32, learning rate 2e-5\n","\n","### 3. **Clustering & Indexing**\n","- Cluster positive/negative examples per rule (max 50 clusters each)\n","- Use FAISS for fast similarity search\n","- Create centroids representing rule-compliant vs. rule-violating patterns\n","\n","### 4. **Prediction**\n","- For each comment: find distance to nearest positive/negative clusters\n","- Score = `-positive_distance + negative_distance` (higher = more violating)\n","- Apply rule-conditioned weighting: multiply by rule relevance (This is just for scaling, not that important tbh)\n","\n","**For those who want a TLDR**: Small BGE model → Triplet Fine-tuning → FAISS Clustering → Cluster Distance Based Scoring\n","\n","## Future Steps:\n","You can experiment without the limitations I set in this notebook, such as small models, subsampling, fast clustering, etc., to achieve more precise results.\n","\n","## Update:\n","\n","- Changed cluster logic.\n","- More pretraining data & slightly larger model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-05T17:16:31.002605Z","iopub.status.busy":"2025-10-05T17:16:31.002298Z","iopub.status.idle":"2025-10-05T17:16:33.473122Z","shell.execute_reply":"2025-10-05T17:16:33.472541Z","shell.execute_reply.started":"2025-10-05T17:16:31.002581Z"},"trusted":true},"outputs":[],"source":["#!/usr/bin/env python3\n","\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","from datasets import Dataset\n","from sentence_transformers import (\n","    SentenceTransformer,\n","    SentenceTransformerTrainer,\n","    SentenceTransformerTrainingArguments,\n","    models\n",")\n","from sentence_transformers.losses import TripletLoss\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","from urllib.parse import urlparse\n","import faiss\n","from tqdm.auto import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","def cleaner(text):\n","    \"\"\"Replace URLs with format: <url>: (domain/important-path)\"\"\"\n","    if not text:\n","        return text\n","\n","    # Regex pattern to match URLs\n","    url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n","\n","    def replace_url(match):\n","        url = match.group(0)\n","        try:\n","            parsed = urlparse(url)\n","            domain = parsed.netloc.lower()\n","            # Remove www. prefix if present\n","            if domain.startswith('www.'):\n","                domain = domain[4:]\n","\n","            # Extract meaningful path parts (first 1-2 segments)\n","            path_parts = [part for part in parsed.path.split('/') if part]\n","            if path_parts:\n","                # Take first 1-2 meaningful path segments\n","                important_path = '/'.join(path_parts[:2])\n","                return f\"<url>: ({domain}/{important_path})\"\n","            else:\n","                return f\"<url>: ({domain})\"\n","        except:\n","            return \"<url>: (unknown)\"\n","\n","    return re.sub(url_pattern, replace_url, str(text))\n","\n","\n","def load_test_data():\n","    \"\"\"Load test data.\"\"\"\n","    print(\"Loading test data...\")\n","    test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n","    print(f\"Loaded {len(test_df)} test examples\")\n","    print(f\"Unique rules: {test_df['rule'].nunique()}\")\n","    return test_df\n","    \n","def load_train_data():\n","    #\"\"Load training data from train.csv.\"\"\"\n","    #print(\"Loading training data...\")\n","    train_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/train.csv')\n","    #print(f\"Loaded {len(train_df)} training examples\")\n","    #print(f\"Unique rules: {train_df['rule'].nunique()}\")\n","    return train_df\n","\n","def collect_all_texts(test_df):\n","    \"\"\"Collect all unique texts from test set.\"\"\"\n","    print(\"\\nCollecting all texts for embedding...\")\n","    \n","    all_texts = set()\n","    \n","    # Add all bodies\n","    for body in test_df['body']:\n","        if pd.notna(body):\n","            all_texts.add(cleaner(str(body)))\n","    \n","    # Add all positive and negative examples\n","    example_cols = ['positive_example_1', 'positive_example_2', \n","                   'negative_example_1', 'negative_example_2']\n","    \n","    for col in example_cols:\n","        for example in test_df[col]:\n","            if pd.notna(example):\n","                all_texts.add(cleaner(str(example)))\n","    \n","    all_texts = list(all_texts)\n","    print(f\"Collected {len(all_texts)} unique texts\")\n","    return all_texts\n","\n","\n","def generate_embeddings(texts, model, batch_size=64):\n","    \"\"\"Generate BGE embeddings for all texts.\"\"\"\n","    print(f\"Generating embeddings for {len(texts)} texts...\")\n","    \n","    embeddings = model.encode(\n","        sentences=texts,\n","        batch_size=batch_size,\n","        show_progress_bar=True,\n","        convert_to_tensor=False,\n","        normalize_embeddings=True\n","    )\n","    \n","    return embeddings\n","\n","\n","def create_test_triplet_dataset(test_df, augmentation_factor=2, random_seed=42, subsample_fraction=1.0):\n","    \"\"\"Create triplet dataset from test data: anchor=rule, positive=positive_example, negative=negative_example.\"\"\"\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    \n","    anchors = []\n","    positives = []\n","    negatives = []\n","    \n","    print(\"Creating rule-aligned triplets from test data...\")\n","    \n","    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test rows\"):\n","        rule = cleaner(str(row['rule']))\n","        \n","        pos_examples = []  # Will contain compliant comments (rule-aligned)\n","        neg_examples = []  # Will contain violating comments (rule-misaligned)\n","\n","        for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n","            if pd.notna(row[neg_col]):\n","                pos_examples.append(cleaner(str(row[neg_col])))\n","\n","        for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n","            if pd.notna(row[pos_col]):\n","                neg_examples.append(cleaner(str(row[pos_col])))\n","        \n","        for pos_ex in pos_examples:\n","            for neg_ex in neg_examples:\n","                anchors.append(rule)\n","                positives.append(pos_ex)\n","                negatives.append(neg_ex)\n","    \n","    if augmentation_factor > 0:\n","        print(f\"Adding {augmentation_factor}x augmentation...\")\n","        \n","        rule_positives = {}\n","        rule_negatives = {}\n","        \n","        for rule in test_df['rule'].unique():\n","            rule_df = test_df[test_df['rule'] == rule]\n","            \n","            pos_pool = []\n","            neg_pool = []\n","            \n","            for _, row in rule_df.iterrows():\n","                for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n","                    if pd.notna(row[neg_col]):\n","                        pos_pool.append(cleaner(str(row[neg_col])))\n","                for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n","                    if pd.notna(row[pos_col]):\n","                        neg_pool.append(cleaner(str(row[pos_col])))\n","            \n","            rule_positives[rule] = list(set(pos_pool))\n","            rule_negatives[rule] = list(set(neg_pool))\n","        \n","        for rule in test_df['rule'].unique():\n","            clean_rule = cleaner(str(rule))\n","            pos_pool = rule_positives[rule]\n","            neg_pool = rule_negatives[rule]\n","            \n","            n_samples = min(augmentation_factor * len(pos_pool), len(pos_pool) * len(neg_pool))\n","            \n","            for _ in range(n_samples):\n","                if pos_pool and neg_pool:\n","                    anchors.append(clean_rule)\n","                    positives.append(random.choice(pos_pool))\n","                    negatives.append(random.choice(neg_pool))\n","    \n","    combined = list(zip(anchors, positives, negatives))\n","    random.shuffle(combined)\n","    \n","    # Apply subsampling if requested\n","    original_count = len(combined)\n","    if subsample_fraction < 1.0:\n","        n_samples = int(len(combined) * subsample_fraction)\n","        combined = combined[:n_samples]\n","        print(f\"Subsampled {original_count} -> {len(combined)} triplets ({subsample_fraction*100:.1f}%)\")\n","    \n","    anchors, positives, negatives = zip(*combined) if combined else ([], [], [])\n","    \n","    print(f\"Created {len(anchors)} triplets from test data\")\n","    \n","    dataset = Dataset.from_dict({\n","        'anchor': list(anchors),\n","        'positive': list(positives),\n","        'negative': list(negatives)\n","    })\n","    \n","    return dataset\n","\n","\n","def fine_tune_model(model, train_dataset, epochs=3, batch_size=32, learning_rate=2e-5, margin=0.25, output_dir=\"./models/test-finetuned-bge\"):\n","    \"\"\"Fine-tune the sentence transformer model using triplet loss on test data.\"\"\"\n","    \n","    print(f\"Fine-tuning model on {len(train_dataset)} triplets...\")\n","    \n","    loss = TripletLoss(model=model, triplet_margin=margin)\n","    \n","    # Calculate max_steps for small datasets\n","    dataset_size = len(train_dataset)\n","    steps_per_epoch = max(1, dataset_size // batch_size)\n","    max_steps = steps_per_epoch * epochs\n","\n","    args = SentenceTransformerTrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=batch_size,\n","        warmup_steps=0,\n","        learning_rate=learning_rate,\n","        logging_steps=max(1, max_steps // 4),\n","        save_strategy=\"epoch\",\n","        save_total_limit=1,\n","        fp16=True,\n","        max_grad_norm=1.0,\n","        dataloader_drop_last=False,\n","        gradient_checkpointing=True,\n","        gradient_accumulation_steps = 1,\n","        max_steps=max_steps,\n","        report_to=\"none\"\n","    )\n","    \n","    trainer = SentenceTransformerTrainer(\n","        model=model,\n","        args=args,\n","        train_dataset=train_dataset,\n","        loss=loss,\n","    )\n","    \n","    trainer.train()\n","    \n","    final_model_path = f\"{output_dir}/final\"\n","    print(f\"Saving fine-tuned model to {final_model_path}...\")\n","    model.save_pretrained(final_model_path)\n","    \n","    return model, final_model_path\n","\n","\n","def load_or_create_finetuned_model(test_df):\n","    \"\"\"Load fine-tuned model if exists, otherwise create and fine-tune it.\"\"\"\n","    \n","    fine_tuned_path = \"./models/test-finetuned-bge/final\"\n","    \n","    if os.path.exists(fine_tuned_path):\n","        print(f\"Loading existing fine-tuned model from {fine_tuned_path}...\")\n","        try:\n","            word_embedding_model = models.Transformer(fine_tuned_path, max_seq_length=128, do_lower_case=True)\n","            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","            model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","            print(\"Loaded fine-tuned model with explicit pooling\")\n","        except:\n","            model = SentenceTransformer(fine_tuned_path)\n","            print(\"Loaded fine-tuned model with default configuration\")\n","        model.half()\n","        return model\n","    \n","    print(\"Fine-tuned model not found. Creating new one...\")\n","    \n","    print(\"Loading base BGE embedding model...\")\n","    # Try Kaggle path first, fallback to HuggingFace\n","    try:\n","        model_path = \"/kaggle/input/baai/transformers/bge-base-en-v1.5/1\"\n","        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n","        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","        print(\"Loaded base model from Kaggle path with explicit pooling\")\n","    except:\n","        model_path = \"\"  # BAAI/bge-small-en-v1.5\n","        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n","        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","        print(\"Loaded base model from local path with explicit pooling\")\n","    \n","    \n","    triplet_dataset = create_test_triplet_dataset(test_df, augmentation_factor=2, subsample_fraction=1.)\n","    \n","    fine_tuned_model, model_path = fine_tune_model(\n","        model=base_model,\n","        train_dataset=triplet_dataset,\n","        epochs=1,\n","        batch_size=32,\n","        learning_rate=2e-5,\n","        margin=0.25\n","    )\n","    \n","    print(f\"Fine-tuning completed. Model saved to: {model_path}\")\n","    fine_tuned_model.half()\n","    return fine_tuned_model\n","\n","\n","def generate_rule_embeddings(test_df, model):\n","    \"\"\"Generate embeddings for each unique rule.\"\"\"\n","    print(\"Generating rule embeddings...\")\n","    \n","    unique_rules = test_df['rule'].unique()\n","    rule_embeddings = {}\n","    \n","    for rule in unique_rules:\n","        clean_rule = cleaner(str(rule))\n","        rule_emb = model.encode(\n","            clean_rule,\n","            convert_to_tensor=False,\n","            normalize_embeddings=True\n","        )\n","        rule_embeddings[rule] = rule_emb\n","        \n","    print(f\"Generated embeddings for {len(rule_embeddings)} rules\")\n","    return rule_embeddings\n","\n","\n","def create_rule_centroids(test_df, text_to_embedding, rule_embeddings):\n","    \"\"\"Create single centroid (mean) for positive and negative examples for each rule.\"\"\"\n","    print(f\"\\nCreating rule centroids (single mean centroid per type)...\")\n","\n","    rule_centroids = {}\n","\n","    for rule in test_df['rule'].unique():\n","        rule_data = test_df[test_df['rule'] == rule]\n","\n","        # Collect positive examples\n","        pos_embeddings = []\n","        for _, row in rule_data.iterrows():\n","            for col in ['positive_example_1', 'positive_example_2']:\n","                if pd.notna(row[col]):\n","                    clean_text = cleaner(str(row[col]))\n","                    if clean_text in text_to_embedding:\n","                        pos_embeddings.append(text_to_embedding[clean_text])\n","\n","        # Collect negative examples\n","        neg_embeddings = []\n","        for _, row in rule_data.iterrows():\n","            for col in ['negative_example_1', 'negative_example_2']:\n","                if pd.notna(row[col]):\n","                    clean_text = cleaner(str(row[col]))\n","                    if clean_text in text_to_embedding:\n","                        neg_embeddings.append(text_to_embedding[clean_text])\n","\n","        if pos_embeddings and neg_embeddings:\n","            pos_embeddings = np.array(pos_embeddings)\n","            neg_embeddings = np.array(neg_embeddings)\n","\n","            # Compute mean centroids\n","            pos_centroid = pos_embeddings.mean(axis=0)\n","            neg_centroid = neg_embeddings.mean(axis=0)\n","\n","            # Normalize centroids\n","            pos_centroid = pos_centroid / np.linalg.norm(pos_centroid)\n","            neg_centroid = neg_centroid / np.linalg.norm(neg_centroid)\n","\n","            rule_centroids[rule] = {\n","                'positive': pos_centroid,\n","                'negative': neg_centroid,\n","                'pos_count': len(pos_embeddings),\n","                'neg_count': len(neg_embeddings),\n","                'rule_embedding': rule_embeddings[rule]\n","            }\n","\n","            print(f\"  Rule: {rule[:50]}... - Pos: {len(pos_embeddings)}, Neg: {len(neg_embeddings)}\")\n","\n","    print(f\"Created centroids for {len(rule_centroids)} rules\")\n","    return rule_centroids\n","\n","\n","def predict_test_set(test_df, text_to_embedding, rule_centroids):\n","    \"\"\"Predict test set using Euclidean distance between body and pos/neg centroids.\"\"\"\n","    print(\"\\nMaking predictions on test set with Euclidean distance...\")\n","\n","    row_ids = []\n","    predictions = []\n","\n","    for rule in test_df['rule'].unique():\n","        print(f\"  Processing rule: {rule[:50]}...\")\n","        rule_data = test_df[test_df['rule'] == rule]\n","\n","        if rule not in rule_centroids:\n","            continue\n","\n","        pos_centroid = rule_centroids[rule]['positive']\n","        neg_centroid = rule_centroids[rule]['negative']\n","\n","        # Collect all valid embeddings and row_ids for this rule\n","        valid_embeddings = []\n","        valid_row_ids = []\n","\n","        for _, row in rule_data.iterrows():\n","            body = cleaner(str(row['body']))\n","            row_id = row['row_id']\n","\n","            if body in text_to_embedding:\n","                valid_embeddings.append(text_to_embedding[body])\n","                valid_row_ids.append(row_id)\n","\n","        if not valid_embeddings:\n","            continue\n","\n","        # Convert to numpy array\n","        query_embeddings = np.array(valid_embeddings)\n","\n","        # Compute Euclidean distances\n","        pos_distances = np.linalg.norm(query_embeddings - pos_centroid, axis=1)\n","        neg_distances = np.linalg.norm(query_embeddings - neg_centroid, axis=1)\n","\n","        # Score: closer to positive (lower distance) = higher violation score\n","        rule_predictions = neg_distances - pos_distances\n","\n","        row_ids.extend(valid_row_ids)\n","        predictions.extend(rule_predictions)\n","\n","    print(f\"Made predictions for {len(predictions)} test examples\")\n","    return row_ids, np.array(predictions)\n","\n","\n","\n","\n","def main():\n","    \"\"\"Main inference pipeline.\"\"\"\n","    print(\"=\"*70)\n","    print(\"SIMPLE SIMILARITY CLASSIFIER - INFERENCE\")\n","    print(\"=\"*70)\n","    \n","    # Step 1: Load test data\n","    test_df = load_test_data()\n","    train_df = load_train_data()\n","    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n","    \n","    # Step 2: Load or create fine-tuned model\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"MODEL PREPARATION PHASE\")\n","    print(\"=\"*50)\n","    model = load_or_create_finetuned_model(combined_df)\n","    \n","    # Step 3: Collect all texts\n","    all_texts = collect_all_texts(test_df)\n","    \n","    # Step 4: Generate embeddings with fine-tuned model\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"EMBEDDING GENERATION PHASE\")\n","    print(\"=\"*50)\n","    all_embeddings = generate_embeddings(all_texts, model)\n","    \n","    # Step 5: Create text to embedding mapping\n","    text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n","    \n","    # Step 6: Generate rule embeddings\n","    rule_embeddings = generate_rule_embeddings(test_df, model)\n","    \n","    # Step 7: Create rule centroids from test examples\n","    rule_centroids = create_rule_centroids(test_df, text_to_embedding, rule_embeddings)\n","    \n","    # Step 8: Predict test set\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"PREDICTION PHASE\")\n","    print(\"=\"*50)\n","    row_ids, predictions = predict_test_set(test_df, text_to_embedding, rule_centroids)\n","    \n","    # Step 9: Create submission with rule-conditioned scores\n","    submission_df = pd.DataFrame({\n","        'row_id': row_ids,\n","        'rule_violation': predictions\n","    })\n","    \n","    submission_df.to_csv('submission.csv', index=False)\n","    print(f\"\\nSaved predictions for {len(submission_df)} test examples to submission.csv\")\n","    \n","    print(f\"\\n{'='*70}\")\n","    print(f\"FINE-TUNED EUCLIDEAN DISTANCE INFERENCE COMPLETED\")\n","    print(f\"Model: Fine-tuned BGE on test data triplets\")\n","    print(f\"Method: Single centroid with Euclidean distance\")\n","    print(f\"Predicted on {len(test_df)} test examples\")\n","    print(f\"Prediction stats: min={predictions.min():.4f}, max={predictions.max():.4f}, mean={predictions.mean():.4f}\")\n","    print(f\"{'='*70}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":13121456,"sourceId":94635,"sourceType":"competition"},{"modelId":128845,"modelInstanceId":104636,"sourceId":124328,"sourceType":"modelInstanceVersion"},{"modelId":128845,"modelInstanceId":104644,"sourceId":124336,"sourceType":"modelInstanceVersion"},{"modelId":128845,"modelInstanceId":104643,"sourceId":124335,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":4}
