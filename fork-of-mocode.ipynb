{"cells":[{"cell_type":"markdown","metadata":{},"source":["# model1 \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-21T10:54:26.862853Z","iopub.status.busy":"2025-10-21T10:54:26.86261Z","iopub.status.idle":"2025-10-21T10:55:14.030493Z","shell.execute_reply":"2025-10-21T10:55:14.029761Z","shell.execute_reply.started":"2025-10-21T10:54:26.862801Z"},"trusted":true},"outputs":[],"source":["!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n","!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'deepspeed==0.17.4' -q\n","!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n","!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n","!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-10-21T10:58:39.673043Z","iopub.status.busy":"2025-10-21T10:58:39.67233Z"},"trusted":true},"outputs":[],"source":["%%writefile constants.py\n","\n","seed = 0\n","\n","base_model_path = \"/kaggle/input/jigsaw-pretrain-public/pytorch/llama-3.2-3b-instruct/1\"\n","pretrain_lora_path = None\n","lora_path = \"/kaggle/working/pseudo_lora\"\n","use_gptq = \"gptq\" in base_model_path\n","\n","positive = \"Yes\"\n","negative = \"No\"\n","judge_words = \"Violation:\"\n","system_prompt = '''You are given a comment from reddit and a rule. \n","Your task is to classify whether the comment violates the rule. \n","Only respond Yes/No.'''\n","\n","frac = 0.05\n","use_train = True\n","\n","import kagglehub\n","\n","deterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\n","deterministic.init_all(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%writefile utils.py\n","\n","import numpy as np\n","import pandas as pd\n","from datasets import Dataset\n","from constants import *\n","\n","def build_prompt(row):\n","    return f\"\"\"{system_prompt}\n","Subreddit: r/{row[\"subreddit\"]}\n","Rule: {row[\"rule\"]}\n","Examples:\n","1) {row[\"positive_example\"]}\n","{judge_words} Yes\n","2) {row[\"negative_example\"]}\n","{judge_words} No\n","Comment: {row[\"body\"]}\n","{judge_words}\"\"\"\n","\n","def get_df():\n","    merge = list()\n","    if use_train:\n","        train_dataset = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\n","        train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n","                                \"positive_example_1\", \"positive_example_2\", \n","                                \"negative_example_1\", \"negative_example_2\"]].copy()\n","        train_df[\"positive_example\"] = np.where(np.random.rand(len(train_df)) < 0.5, train_df[\"positive_example_1\"], train_df[\"positive_example_2\"])\n","        train_df[\"negative_example\"] = np.where(np.random.rand(len(train_df)) < 0.5, train_df[\"negative_example_1\"], train_df[\"negative_example_2\"])\n","        train_df.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], inplace=True)\n","        merge.append(train_df)\n","    test_dataset = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n","    test_dataset = test_dataset.groupby('rule', group_keys=False).apply(lambda x: x.sample(frac=frac, random_state=seed)).reset_index(drop=True)\n","    print(f\"Select {len(test_dataset)} test data\")\n","    for violation_type in [\"positive\", \"negative\"]:\n","        for i in range(1, 3):\n","            sub_dataset = test_dataset[[\"rule\", \"subreddit\", \"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"]].copy()\n","            body_col = f\"{violation_type}_example_{i}\"\n","            other_positive_col = f\"{violation_type}_example_{3-i}\"\n","            sub_dataset[\"body\"] = sub_dataset[body_col]\n","            sub_dataset[f\"{violation_type}_example\"] = sub_dataset[other_positive_col]\n","            anti_violation_type = \"negative\" if violation_type == \"positive\" else \"positive\"\n","            sub_dataset[f\"{anti_violation_type}_example\"] = np.where(np.random.rand(len(sub_dataset)) < 0.5, sub_dataset[f\"{anti_violation_type}_example_1\"], sub_dataset[f\"{anti_violation_type}_example_2\"])\n","            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n","            sub_dataset.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], inplace=True)\n","            merge.append(sub_dataset)\n","    return pd.concat(merge, axis=0).drop_duplicates(ignore_index=True)\n","\n","def build_dataset(df):\n","    df[\"prompt\"] = df.apply(build_prompt, axis=1)\n","    columns = [\"prompt\"]\n","    if \"rule_violation\" in df:\n","        df[\"completion\"] = df[\"rule_violation\"].map({\n","            1: positive,\n","            0: negative,})\n","        columns.append(\"completion\")\n","    dataset = Dataset.from_pandas(df[columns])\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%writefile train.py\n","\n","import torch\n","import pandas as pd\n","from trl import SFTTrainer, SFTConfig\n","from peft import PeftModel, LoraConfig, get_peft_model\n","from tqdm.auto import tqdm\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from transformers.utils import is_torch_bf16_gpu_available\n","\n","from utils import *\n","from constants import *\n","\n","def main():\n","    train_dataset = build_dataset(get_df())\n","    lora_config = LoraConfig(\n","        r=64,\n","        lora_alpha=128,\n","        lora_dropout=0.1,\n","        bias=\"none\",\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","        task_type=\"CAUSAL_LM\",\n","    )\n","    \n","    training_args = SFTConfig(\n","        num_train_epochs=1,\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=4,\n","        optim=\"paged_adamw_8bit\",\n","        learning_rate=1e-4,\n","        weight_decay=0.01,\n","        max_grad_norm=1.0,\n","        lr_scheduler_type=\"cosine\",\n","        warmup_ratio=0.03,\n","        bf16=is_torch_bf16_gpu_available(),\n","        fp16=not is_torch_bf16_gpu_available(),\n","        dataloader_pin_memory=True,\n","        gradient_checkpointing=True,\n","        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","        save_strategy=\"no\",\n","        report_to=\"none\",\n","        completion_only_loss=True,\n","        packing=False,\n","        remove_unused_columns=False,\n","    )\n","\n","    if use_gptq:\n","        model = AutoModelForCausalLM.from_pretrained(\n","            base_model_path,\n","            device_map=\"balanced_low_0\",\n","            trust_remote_code=True,\n","            use_cache=False,\n","        )\n","    else:\n","        model = AutoModelForCausalLM.from_pretrained(\n","            base_model_path,\n","            quantization_config=BitsAndBytesConfig(\n","                load_in_4bit=True,     \n","                bnb_4bit_quant_type=\"nf4\",\n","                bnb_4bit_compute_dtype=torch.bfloat16,\n","                bnb_4bit_use_double_quant=True,\n","            ),\n","            device_map=\"balanced_low_0\",\n","            trust_remote_code=True,\n","            use_cache=False,\n","        )\n","    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    if pretrain_lora_path:\n","        model = PeftModel.from_pretrained(model, pretrain_lora_path)\n","        model = model.merge_and_unload()\n","\n","    if len(train_dataset) > 0:\n","        trainer = SFTTrainer(\n","            model=model,\n","            processing_class=tokenizer,\n","            args=training_args,\n","            train_dataset=train_dataset,\n","            peft_config=lora_config,\n","        )\n","        trainer.train()\n","        trainer.save_model(lora_path)\n","    else:\n","        peft_model = get_peft_model(model, lora_config)\n","        peft_model.save_pretrained(lora_path)\n","        tokenizer.save_pretrained(lora_path)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%writefile accelerate_config.yaml\n","compute_environment: LOCAL_MACHINE\n","debug: false\n","deepspeed_config:\n","  gradient_accumulation_steps: 4\n","  gradient_clipping: 1.0\n","  train_micro_batch_size_per_gpu: 4\n","  \n","  zero_stage: 2\n","  offload_optimizer_device: none\n","  offload_param_device: none\n","  zero3_init_flag: false\n","  \n","  stage3_gather_16bit_weights_on_model_save: false\n","  stage3_max_live_parameters: 1e8\n","  stage3_max_reuse_distance: 1e8\n","  stage3_prefetch_bucket_size: 5e7\n","  stage3_param_persistence_threshold: 1e5\n","  \n","  zero_allow_untested_optimizer: true\n","  zero_force_ds_cpu_optimizer: false\n","  \n","  # fp16:\n","  #   enabled: true\n","  #   loss_scale: 0\n","  #   initial_scale_power: 16\n","  #   loss_scale_window: 1000\n","  #   hysteresis: 2\n","  #   min_loss_scale: 1\n","  bf16:\n","    enabled: true\n","  \n","distributed_type: DEEPSPEED\n","downcast_bf16: 'yes'\n","dynamo_config:\n","  dynamo_backend: INDUCTOR\n","  dynamo_use_fullgraph: false\n","  dynamo_use_dynamic: false\n","enable_cpu_affinity: false\n","machine_rank: 0\n","main_training_function: main\n","mixed_precision: fp16\n","num_machines: 1\n","num_processes: 2\n","rdzv_backend: static\n","same_network: true\n","tpu_env: []\n","tpu_use_cluster: false\n","tpu_use_sudo: false\n","use_cpu: false"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%writefile inference.py\n","\n","import os\n","os.environ[\"VLLM_USE_V1\"] = \"0\"\n","\n","import random\n","import vllm\n","import torch\n","import numpy as np\n","import pandas as pd\n","from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n","from vllm.lora.request import LoRARequest\n","from utils import build_dataset\n","from constants import *\n","import multiprocessing as mp\n","\n","def run_inference_on_device(df_slice):\n","    llm = vllm.LLM(\n","        base_model_path,\n","        quantization=\"gptq\" if use_gptq else None,\n","        tensor_parallel_size=1,\n","        gpu_memory_utilization=0.98,\n","        trust_remote_code=True,\n","        dtype=\"half\",\n","        enforce_eager=True,\n","        max_model_len=2048,\n","        disable_log_stats=True,\n","        enable_prefix_caching=True,\n","        enable_lora=True,\n","        max_lora_rank=64,\n","    )\n","    tokenizer = llm.get_tokenizer()\n","    outputs = llm.generate(\n","        build_dataset(df_slice)[\"prompt\"],\n","        vllm.SamplingParams(\n","            skip_special_tokens=True,\n","            max_tokens=1,\n","            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[positive, negative])],\n","            logprobs=2,\n","        ),\n","        use_tqdm=True,\n","        lora_request=LoRARequest(\"lora1\", 1, lora_path)\n","    )\n","    log_probs = [{lp.decoded_token: np.exp(lp.logprob) for lp in out.outputs[0].logprobs[0].values()} for out in outputs]\n","    predictions = pd.DataFrame(log_probs)[[positive, negative]]\n","    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n","    return predictions\n","\n","def worker(device_id, df_slice, return_dict):\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n","    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n","    preds = run_inference_on_device(df_slice)\n","    return_dict[device_id] = preds\n","\n","def main():\n","    test_df = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n","    test_df[\"positive_example\"] = test_df.apply(lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1)\n","    test_df[\"negative_example\"] = test_df.apply(lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1)\n","    test_df = test_df.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], errors=\"ignore\")\n","\n","    mid = len(test_df) // 2\n","    df0 = test_df.iloc[:mid].reset_index(drop=True)\n","    df1 = test_df.iloc[mid:].reset_index(drop=True)\n","\n","    manager = mp.Manager()\n","    return_dict = manager.dict()\n","    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n","    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n","    p0.start()\n","    p1.start()\n","    p0.join()\n","    p1.join()\n","\n","    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n","    submission = predictions[[\"row_id\", positive]].rename(columns={positive: \"rule_violation\"})\n","    submission.to_csv(\"/kaggle/working/submission5.csv\", index=False)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!accelerate launch --config_file accelerate_config.yaml train.py\n","!python inference.py\n","\n","import pandas as pd\n","pd.read_csv('/kaggle/working/submission5.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# model2 \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2025-10-21T10:55:30.009993Z","iopub.status.idle":"2025-10-21T10:55:30.010213Z","shell.execute_reply":"2025-10-21T10:55:30.01011Z","shell.execute_reply.started":"2025-10-21T10:55:30.010101Z"},"trusted":true},"outputs":[],"source":["#!/usr/bin/env python3\n","\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","from datasets import Dataset\n","from sentence_transformers import (\n","    SentenceTransformer,\n","    SentenceTransformerTrainer,\n","    SentenceTransformerTrainingArguments,\n","    models\n",")\n","from sentence_transformers.losses import TripletLoss\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","from urllib.parse import urlparse\n","import faiss\n","from tqdm.auto import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","def cleaner(text):\n","    \"\"\"Replace URLs with format: <url>: (domain/important-path)\"\"\"\n","    if not text:\n","        return text\n","\n","    # Regex pattern to match URLs\n","    url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n","\n","    def replace_url(match):\n","        url = match.group(0)\n","        try:\n","            parsed = urlparse(url)\n","            domain = parsed.netloc.lower()\n","            # Remove www. prefix if present\n","            if domain.startswith('www.'):\n","                domain = domain[4:]\n","\n","            # Extract meaningful path parts (first 1-2 segments)\n","            path_parts = [part for part in parsed.path.split('/') if part]\n","            if path_parts:\n","                # Take first 1-2 meaningful path segments\n","                important_path = '/'.join(path_parts[:2])\n","                return f\"<url>: ({domain}/{important_path})\"\n","            else:\n","                return f\"<url>: ({domain})\"\n","        except:\n","            return \"<url>: (unknown)\"\n","\n","    return re.sub(url_pattern, replace_url, str(text))\n","\n","\n","def load_test_data():\n","    \"\"\"Load test data.\"\"\"\n","    print(\"Loading test data...\")\n","    test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n","    print(f\"Loaded {len(test_df)} test examples\")\n","    print(f\"Unique rules: {test_df['rule'].nunique()}\")\n","    return test_df\n","\n","\n","def collect_all_texts(test_df):\n","    \"\"\"Collect all unique texts from test set.\"\"\"\n","    print(\"\\nCollecting all texts for embedding...\")\n","    \n","    all_texts = set()\n","    \n","    # Add all bodies\n","    for body in test_df['body']:\n","        if pd.notna(body):\n","            all_texts.add(cleaner(str(body)))\n","    \n","    # Add all positive and negative examples\n","    example_cols = ['positive_example_1', 'positive_example_2', \n","                   'negative_example_1', 'negative_example_2']\n","    \n","    for col in example_cols:\n","        for example in test_df[col]:\n","            if pd.notna(example):\n","                all_texts.add(cleaner(str(example)))\n","    \n","    all_texts = list(all_texts)\n","    print(f\"Collected {len(all_texts)} unique texts\")\n","    return all_texts\n","\n","\n","def generate_embeddings(texts, model, batch_size=64):\n","    \"\"\"Generate BGE embeddings for all texts.\"\"\"\n","    print(f\"Generating embeddings for {len(texts)} texts...\")\n","    \n","    embeddings = model.encode(\n","        sentences=texts,\n","        batch_size=batch_size,\n","        show_progress_bar=True,\n","        convert_to_tensor=False,\n","        normalize_embeddings=True\n","    )\n","    \n","    return embeddings\n","\n","\n","def create_test_triplet_dataset(test_df, augmentation_factor=2, random_seed=42, subsample_fraction=1.0):\n","    \"\"\"Create triplet dataset from test data: anchor=rule, positive=positive_example, negative=negative_example.\"\"\"\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    \n","    anchors = []\n","    positives = []\n","    negatives = []\n","    \n","    print(\"Creating rule-aligned triplets from test data...\")\n","    \n","    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test rows\"):\n","        rule = cleaner(str(row['rule']))\n","        \n","        pos_examples = []  # Will contain compliant comments (rule-aligned)\n","        neg_examples = []  # Will contain violating comments (rule-misaligned)\n","\n","        for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n","            if pd.notna(row[neg_col]):\n","                pos_examples.append(cleaner(str(row[neg_col])))\n","\n","        for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n","            if pd.notna(row[pos_col]):\n","                neg_examples.append(cleaner(str(row[pos_col])))\n","        \n","        for pos_ex in pos_examples:\n","            for neg_ex in neg_examples:\n","                anchors.append(rule)\n","                positives.append(pos_ex)\n","                negatives.append(neg_ex)\n","    \n","    if augmentation_factor > 0:\n","        print(f\"Adding {augmentation_factor}x augmentation...\")\n","        \n","        rule_positives = {}\n","        rule_negatives = {}\n","        \n","        for rule in test_df['rule'].unique():\n","            rule_df = test_df[test_df['rule'] == rule]\n","            \n","            pos_pool = []\n","            neg_pool = []\n","            \n","            for _, row in rule_df.iterrows():\n","                for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n","                    if pd.notna(row[neg_col]):\n","                        pos_pool.append(cleaner(str(row[neg_col])))\n","                for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n","                    if pd.notna(row[pos_col]):\n","                        neg_pool.append(cleaner(str(row[pos_col])))\n","            \n","            rule_positives[rule] = list(set(pos_pool))\n","            rule_negatives[rule] = list(set(neg_pool))\n","        \n","        for rule in test_df['rule'].unique():\n","            clean_rule = cleaner(str(rule))\n","            pos_pool = rule_positives[rule]\n","            neg_pool = rule_negatives[rule]\n","            \n","            n_samples = min(augmentation_factor * len(pos_pool), len(pos_pool) * len(neg_pool))\n","            \n","            for _ in range(n_samples):\n","                if pos_pool and neg_pool:\n","                    anchors.append(clean_rule)\n","                    positives.append(random.choice(pos_pool))\n","                    negatives.append(random.choice(neg_pool))\n","    \n","    combined = list(zip(anchors, positives, negatives))\n","    random.shuffle(combined)\n","    \n","    # Apply subsampling if requested\n","    original_count = len(combined)\n","    if subsample_fraction < 1.0:\n","        n_samples = int(len(combined) * subsample_fraction)\n","        combined = combined[:n_samples]\n","        print(f\"Subsampled {original_count} -> {len(combined)} triplets ({subsample_fraction*100:.1f}%)\")\n","    \n","    anchors, positives, negatives = zip(*combined) if combined else ([], [], [])\n","    \n","    print(f\"Created {len(anchors)} triplets from test data\")\n","    \n","    dataset = Dataset.from_dict({\n","        'anchor': list(anchors),\n","        'positive': list(positives),\n","        'negative': list(negatives)\n","    })\n","    \n","    return dataset\n","\n","\n","def fine_tune_model(model, train_dataset, epochs=3, batch_size=32, learning_rate=2e-5, margin=0.25, output_dir=\"./models/test-finetuned-bge\"):\n","    \"\"\"Fine-tune the sentence transformer model using triplet loss on test data.\"\"\"\n","    \n","    print(f\"Fine-tuning model on {len(train_dataset)} triplets...\")\n","    \n","    loss = TripletLoss(model=model, triplet_margin=margin)\n","    \n","    # Calculate max_steps for small datasets\n","    dataset_size = len(train_dataset)\n","    steps_per_epoch = max(1, dataset_size // batch_size)\n","    max_steps = steps_per_epoch * epochs\n","\n","    args = SentenceTransformerTrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=batch_size,\n","        warmup_steps=0,\n","        learning_rate=learning_rate,\n","        logging_steps=max(1, max_steps // 4),\n","        save_strategy=\"epoch\",\n","        save_total_limit=1,\n","        fp16=True,\n","        max_grad_norm=1.0,\n","        dataloader_drop_last=False,\n","        gradient_checkpointing=True,\n","        gradient_accumulation_steps = 1,\n","        max_steps=max_steps,\n","        report_to=\"none\"\n","    )\n","    \n","    trainer = SentenceTransformerTrainer(\n","        model=model,\n","        args=args,\n","        train_dataset=train_dataset,\n","        loss=loss,\n","    )\n","    \n","    trainer.train()\n","    \n","    final_model_path = f\"{output_dir}/final\"\n","    print(f\"Saving fine-tuned model to {final_model_path}...\")\n","    model.save_pretrained(final_model_path)\n","    \n","    return model, final_model_path\n","\n","\n","def load_or_create_finetuned_model(test_df):\n","    \"\"\"Load fine-tuned model if exists, otherwise create and fine-tune it.\"\"\"\n","    \n","    fine_tuned_path = \"./models/test-finetuned-bge/final2\"\n","    \n","    if os.path.exists(fine_tuned_path):\n","        print(f\"Loading existing fine-tuned model from {fine_tuned_path}...\")\n","        try:\n","            word_embedding_model = models.Transformer(fine_tuned_path, max_seq_length=128, do_lower_case=True)\n","            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","            model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","            print(\"Loaded fine-tuned model with explicit pooling\")\n","        except:\n","            model = SentenceTransformer(fine_tuned_path)\n","            print(\"Loaded fine-tuned model with default configuration\")\n","        model.half()\n","        return model\n","    \n","    print(\"Fine-tuned model not found. Creating new one...\")\n","    \n","    print(\"Loading base BGE embedding model...\")\n","    # Try Kaggle path first, fallback to HuggingFace\n","    try:\n","        model_path = \"/kaggle/input/baai/transformers/bge-large-en-v1.5/1\"\n","        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n","        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","        print(\"Loaded base model from Kaggle path with explicit pooling\")\n","    except:\n","        model_path = \"\"  # BAAI/bge-small-en-v1.5\n","        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n","        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","        print(\"Loaded base model from local path with explicit pooling\")\n","    \n","    \n","    triplet_dataset = create_test_triplet_dataset(test_df, augmentation_factor=2, subsample_fraction=1.)\n","    \n","    fine_tuned_model, model_path = fine_tune_model(\n","        model=base_model,\n","        train_dataset=triplet_dataset,\n","        epochs=1,\n","        batch_size=32,\n","        learning_rate=2e-5,\n","        margin=0.25\n","    )\n","    \n","    print(f\"Fine-tuning completed. Model saved to: {model_path}\")\n","    fine_tuned_model.half()\n","    return fine_tuned_model\n","\n","\n","def generate_rule_embeddings(test_df, model):\n","    \"\"\"Generate embeddings for each unique rule.\"\"\"\n","    print(\"Generating rule embeddings...\")\n","    \n","    unique_rules = test_df['rule'].unique()\n","    rule_embeddings = {}\n","    \n","    for rule in unique_rules:\n","        clean_rule = cleaner(str(rule))\n","        rule_emb = model.encode(\n","            clean_rule,\n","            convert_to_tensor=False,\n","            normalize_embeddings=True\n","        )\n","        rule_embeddings[rule] = rule_emb\n","        \n","    print(f\"Generated embeddings for {len(rule_embeddings)} rules\")\n","    return rule_embeddings\n","\n","\n","def create_rule_centroids(test_df, text_to_embedding, rule_embeddings):\n","    \"\"\"Create single centroid (mean) for positive and negative examples for each rule.\"\"\"\n","    print(f\"\\nCreating rule centroids (single mean centroid per type)...\")\n","\n","    rule_centroids = {}\n","\n","    for rule in test_df['rule'].unique():\n","        rule_data = test_df[test_df['rule'] == rule]\n","\n","        # Collect positive examples\n","        pos_embeddings = []\n","        for _, row in rule_data.iterrows():\n","            for col in ['positive_example_1', 'positive_example_2']:\n","                if pd.notna(row[col]):\n","                    clean_text = cleaner(str(row[col]))\n","                    if clean_text in text_to_embedding:\n","                        pos_embeddings.append(text_to_embedding[clean_text])\n","\n","        # Collect negative examples\n","        neg_embeddings = []\n","        for _, row in rule_data.iterrows():\n","            for col in ['negative_example_1', 'negative_example_2']:\n","                if pd.notna(row[col]):\n","                    clean_text = cleaner(str(row[col]))\n","                    if clean_text in text_to_embedding:\n","                        neg_embeddings.append(text_to_embedding[clean_text])\n","\n","        if pos_embeddings and neg_embeddings:\n","            pos_embeddings = np.array(pos_embeddings)\n","            neg_embeddings = np.array(neg_embeddings)\n","\n","            # Compute mean centroids\n","            pos_centroid = pos_embeddings.mean(axis=0)\n","            neg_centroid = neg_embeddings.mean(axis=0)\n","\n","            # Normalize centroids\n","            pos_centroid = pos_centroid / np.linalg.norm(pos_centroid)\n","            neg_centroid = neg_centroid / np.linalg.norm(neg_centroid)\n","\n","            rule_centroids[rule] = {\n","                'positive': pos_centroid,\n","                'negative': neg_centroid,\n","                'pos_count': len(pos_embeddings),\n","                'neg_count': len(neg_embeddings),\n","                'rule_embedding': rule_embeddings[rule]\n","            }\n","\n","            print(f\"  Rule: {rule[:50]}... - Pos: {len(pos_embeddings)}, Neg: {len(neg_embeddings)}\")\n","\n","    print(f\"Created centroids for {len(rule_centroids)} rules\")\n","    return rule_centroids\n","\n","\n","def predict_test_set(test_df, text_to_embedding, rule_centroids):\n","    \"\"\"Predict test set using Euclidean distance between body and pos/neg centroids.\"\"\"\n","    print(\"\\nMaking predictions on test set with Euclidean distance...\")\n","\n","    row_ids = []\n","    predictions = []\n","\n","    for rule in test_df['rule'].unique():\n","        print(f\"  Processing rule: {rule[:50]}...\")\n","        rule_data = test_df[test_df['rule'] == rule]\n","\n","        if rule not in rule_centroids:\n","            continue\n","\n","        pos_centroid = rule_centroids[rule]['positive']\n","        neg_centroid = rule_centroids[rule]['negative']\n","\n","        # Collect all valid embeddings and row_ids for this rule\n","        valid_embeddings = []\n","        valid_row_ids = []\n","\n","        for _, row in rule_data.iterrows():\n","            body = cleaner(str(row['body']))\n","            row_id = row['row_id']\n","\n","            if body in text_to_embedding:\n","                valid_embeddings.append(text_to_embedding[body])\n","                valid_row_ids.append(row_id)\n","\n","        if not valid_embeddings:\n","            continue\n","\n","        # Convert to numpy array\n","        query_embeddings = np.array(valid_embeddings)\n","\n","        # Compute Euclidean distances\n","        pos_distances = np.linalg.norm(query_embeddings - pos_centroid, axis=1)\n","        neg_distances = np.linalg.norm(query_embeddings - neg_centroid, axis=1)\n","\n","        # Score: closer to positive (lower distance) = higher violation score\n","        rule_predictions = neg_distances - pos_distances\n","\n","        row_ids.extend(valid_row_ids)\n","        predictions.extend(rule_predictions)\n","\n","    print(f\"Made predictions for {len(predictions)} test examples\")\n","    return row_ids, np.array(predictions)\n","\n","\n","\n","\n","def main():\n","    \"\"\"Main inference pipeline.\"\"\"\n","    print(\"=\"*70)\n","    print(\"SIMPLE SIMILARITY CLASSIFIER - INFERENCE\")\n","    print(\"=\"*70)\n","    \n","    # Step 1: Load test data\n","    test_df = load_test_data()\n","    \n","    # Step 2: Load or create fine-tuned model\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"MODEL PREPARATION PHASE\")\n","    print(\"=\"*50)\n","    model = load_or_create_finetuned_model(test_df)\n","    \n","    # Step 3: Collect all texts\n","    all_texts = collect_all_texts(test_df)\n","    \n","    # Step 4: Generate embeddings with fine-tuned model\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"EMBEDDING GENERATION PHASE\")\n","    print(\"=\"*50)\n","    all_embeddings = generate_embeddings(all_texts, model)\n","    \n","    # Step 5: Create text to embedding mapping\n","    text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n","    \n","    # Step 6: Generate rule embeddings\n","    rule_embeddings = generate_rule_embeddings(test_df, model)\n","    \n","    # Step 7: Create rule centroids from test examples\n","    rule_centroids = create_rule_centroids(test_df, text_to_embedding, rule_embeddings)\n","    \n","    # Step 8: Predict test set\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"PREDICTION PHASE\")\n","    print(\"=\"*50)\n","    row_ids, predictions = predict_test_set(test_df, text_to_embedding, rule_centroids)\n","    \n","    min_vals = predictions.min(axis=0, keepdims=True)  #\n","    max_vals = predictions.max(axis=0, keepdims=True)  #\n","    range_vals = max_vals - min_vals\n","    range_vals = np.where(range_vals == 0, 1, range_vals)\n","    normalized = (predictions - min_vals) / range_vals\n","    \n","    # Step 9: Create submission with rule-conditioned scores\n","    submission_df = pd.DataFrame({\n","        'row_id': row_ids,\n","        'rule_violation': normalized\n","    })\n","    \n","    \n","    submission_df.to_csv('submission1.csv', index=False)\n","    print(f\"\\nSaved predictions for {len(submission_df)} test examples to submission1.csv\")\n","    \n","    print(f\"\\n{'='*70}\")\n","    print(f\"FINE-TUNED EUCLIDEAN DISTANCE INFERENCE COMPLETED\")\n","    print(f\"Model: Fine-tuned BGE on test data triplets\")\n","    print(f\"Method: Single centroid with Euclidean distance\")\n","    print(f\"Predicted on {len(test_df)} test examples\")\n","    print(f\"Prediction stats: min={predictions.min():.4f}, max={predictions.max():.4f}, mean={predictions.mean():.4f}\")\n","    print(f\"{'='*70}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{},"source":["# model3 "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#!/usr/bin/env python3\n","\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","from datasets import Dataset\n","from sentence_transformers import (\n","    SentenceTransformer,\n","    SentenceTransformerTrainer,\n","    SentenceTransformerTrainingArguments,\n","    models\n",")\n","from sentence_transformers.losses import TripletLoss\n","from sklearn.metrics.pairwise import cosine_similarity\n","import re\n","from urllib.parse import urlparse\n","import faiss\n","from tqdm.auto import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","def cleaner(text):\n","    \"\"\"Replace URLs with format: <url>: (domain/important-path)\"\"\"\n","    if not text:\n","        return text\n","\n","    # Regex pattern to match URLs\n","    url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n","\n","    def replace_url(match):\n","        url = match.group(0)\n","        try:\n","            parsed = urlparse(url)\n","            domain = parsed.netloc.lower()\n","            # Remove www. prefix if present\n","            if domain.startswith('www.'):\n","                domain = domain[4:]\n","\n","            # Extract meaningful path parts (first 1-2 segments)\n","            path_parts = [part for part in parsed.path.split('/') if part]\n","            if path_parts:\n","                # Take first 1-2 meaningful path segments\n","                important_path = '/'.join(path_parts[:2])\n","                return f\"<url>: ({domain}/{important_path})\"\n","            else:\n","                return f\"<url>: ({domain})\"\n","        except:\n","            return \"<url>: (unknown)\"\n","\n","    return re.sub(url_pattern, replace_url, str(text))\n","\n","\n","def load_test_data():\n","    \"\"\"Load test data.\"\"\"\n","    print(\"Loading test data...\")\n","    test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n","    print(f\"Loaded {len(test_df)} test examples\")\n","    print(f\"Unique rules: {test_df['rule'].nunique()}\")\n","    return test_df\n","\n","\n","def collect_all_texts(test_df):\n","    \"\"\"Collect all unique texts from test set.\"\"\"\n","    print(\"\\nCollecting all texts for embedding...\")\n","    \n","    all_texts = set()\n","    \n","    # Add all bodies\n","    for body in test_df['body']:\n","        if pd.notna(body):\n","            all_texts.add(cleaner(str(body)))\n","    \n","    # Add all positive and negative examples\n","    example_cols = ['positive_example_1', 'positive_example_2', \n","                   'negative_example_1', 'negative_example_2']\n","    \n","    for col in example_cols:\n","        for example in test_df[col]:\n","            if pd.notna(example):\n","                all_texts.add(cleaner(str(example)))\n","    \n","    all_texts = list(all_texts)\n","    print(f\"Collected {len(all_texts)} unique texts\")\n","    return all_texts\n","\n","\n","def generate_embeddings(texts, model, batch_size=64):\n","    \"\"\"Generate BGE embeddings for all texts.\"\"\"\n","    print(f\"Generating embeddings for {len(texts)} texts...\")\n","    \n","    embeddings = model.encode(\n","        sentences=texts,\n","        batch_size=batch_size,\n","        show_progress_bar=True,\n","        convert_to_tensor=False,\n","        normalize_embeddings=True\n","    )\n","    \n","    return embeddings\n","\n","\n","def create_test_triplet_dataset(test_df, augmentation_factor=2, random_seed=42, subsample_fraction=1.0):\n","    \"\"\"Create triplet dataset from test data: anchor=rule, positive=positive_example, negative=negative_example.\"\"\"\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    \n","    anchors = []\n","    positives = []\n","    negatives = []\n","    \n","    print(\"Creating rule-aligned triplets from test data...\")\n","    \n","    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test rows\"):\n","        rule = cleaner(str(row['rule']))\n","        \n","        pos_examples = []  # Will contain compliant comments (rule-aligned)\n","        neg_examples = []  # Will contain violating comments (rule-misaligned)\n","\n","        for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n","            if pd.notna(row[neg_col]):\n","                pos_examples.append(cleaner(str(row[neg_col])))\n","\n","        for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n","            if pd.notna(row[pos_col]):\n","                neg_examples.append(cleaner(str(row[pos_col])))\n","        \n","        for pos_ex in pos_examples:\n","            for neg_ex in neg_examples:\n","                anchors.append(rule)\n","                positives.append(pos_ex)\n","                negatives.append(neg_ex)\n","    \n","    if augmentation_factor > 0:\n","        print(f\"Adding {augmentation_factor}x augmentation...\")\n","        \n","        rule_positives = {}\n","        rule_negatives = {}\n","        \n","        for rule in test_df['rule'].unique():\n","            rule_df = test_df[test_df['rule'] == rule]\n","            \n","            pos_pool = []\n","            neg_pool = []\n","            \n","            for _, row in rule_df.iterrows():\n","                for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n","                    if pd.notna(row[neg_col]):\n","                        pos_pool.append(cleaner(str(row[neg_col])))\n","                for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n","                    if pd.notna(row[pos_col]):\n","                        neg_pool.append(cleaner(str(row[pos_col])))\n","            \n","            rule_positives[rule] = list(set(pos_pool))\n","            rule_negatives[rule] = list(set(neg_pool))\n","        \n","        for rule in test_df['rule'].unique():\n","            clean_rule = cleaner(str(rule))\n","            pos_pool = rule_positives[rule]\n","            neg_pool = rule_negatives[rule]\n","            \n","            n_samples = min(augmentation_factor * len(pos_pool), len(pos_pool) * len(neg_pool))\n","            \n","            for _ in range(n_samples):\n","                if pos_pool and neg_pool:\n","                    anchors.append(clean_rule)\n","                    positives.append(random.choice(pos_pool))\n","                    negatives.append(random.choice(neg_pool))\n","    \n","    combined = list(zip(anchors, positives, negatives))\n","    random.shuffle(combined)\n","    \n","    # Apply subsampling if requested\n","    original_count = len(combined)\n","    if subsample_fraction < 1.0:\n","        n_samples = int(len(combined) * subsample_fraction)\n","        combined = combined[:n_samples]\n","        print(f\"Subsampled {original_count} -> {len(combined)} triplets ({subsample_fraction*100:.1f}%)\")\n","    \n","    anchors, positives, negatives = zip(*combined) if combined else ([], [], [])\n","    \n","    print(f\"Created {len(anchors)} triplets from test data\")\n","    \n","    dataset = Dataset.from_dict({\n","        'anchor': list(anchors),\n","        'positive': list(positives),\n","        'negative': list(negatives)\n","    })\n","    \n","    return dataset\n","\n","\n","def fine_tune_model(model, train_dataset, epochs=3, batch_size=32, learning_rate=2e-5, margin=0.25, output_dir=\"./models/test-finetuned-bge\"):\n","    \"\"\"Fine-tune the sentence transformer model using triplet loss on test data.\"\"\"\n","    \n","    print(f\"Fine-tuning model on {len(train_dataset)} triplets...\")\n","    \n","    loss = TripletLoss(model=model, triplet_margin=margin)\n","    \n","    # Calculate max_steps for small datasets\n","    dataset_size = len(train_dataset)\n","    steps_per_epoch = max(1, dataset_size // batch_size)\n","    max_steps = steps_per_epoch * epochs\n","\n","    args = SentenceTransformerTrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=epochs,\n","        per_device_train_batch_size=batch_size,\n","        warmup_steps=0,\n","        learning_rate=learning_rate,\n","        logging_steps=max(1, max_steps // 4),\n","        save_strategy=\"epoch\",\n","        save_total_limit=1,\n","        fp16=True,\n","        max_grad_norm=1.0,\n","        dataloader_drop_last=False,\n","        gradient_checkpointing=True,\n","        gradient_accumulation_steps = 1,\n","        max_steps=max_steps,\n","        report_to=\"none\"\n","    )\n","    \n","    trainer = SentenceTransformerTrainer(\n","        model=model,\n","        args=args,\n","        train_dataset=train_dataset,\n","        loss=loss,\n","    )\n","    \n","    trainer.train()\n","    \n","    final_model_path = f\"{output_dir}/final\"\n","    print(f\"Saving fine-tuned model to {final_model_path}...\")\n","    model.save_pretrained(final_model_path)\n","    \n","    return model, final_model_path\n","\n","\n","def load_or_create_finetuned_model(test_df):\n","    \"\"\"Load fine-tuned model if exists, otherwise create and fine-tune it.\"\"\"\n","    \n","    fine_tuned_path = \"./models/test-finetuned-bge/final3\"\n","    \n","    if os.path.exists(fine_tuned_path):\n","        print(f\"Loading existing fine-tuned model from {fine_tuned_path}...\")\n","        try:\n","            word_embedding_model = models.Transformer(fine_tuned_path, max_seq_length=128, do_lower_case=True)\n","            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","            model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","            print(\"Loaded fine-tuned model with explicit pooling\")\n","        except:\n","            model = SentenceTransformer(fine_tuned_path)\n","            print(\"Loaded fine-tuned model with default configuration\")\n","        model.half()\n","        return model\n","    \n","    print(\"Fine-tuned model not found. Creating new one...\")\n","    \n","    print(\"Loading base BGE embedding model...\")\n","    # Try Kaggle path first, fallback to HuggingFace\n","    try:\n","        model_path = \"/kaggle/input/baai/transformers/bge-base-en-v1.5/1\"\n","        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n","        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","        print(\"Loaded base model from Kaggle path with explicit pooling\")\n","    except:\n","        model_path = \"\"  # BAAI/bge-small-en-v1.5\n","        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n","        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n","        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","        print(\"Loaded base model from local path with explicit pooling\")\n","    \n","    \n","    triplet_dataset = create_test_triplet_dataset(test_df, augmentation_factor=2, subsample_fraction=1.)\n","    \n","    fine_tuned_model, model_path = fine_tune_model(\n","        model=base_model,\n","        train_dataset=triplet_dataset,\n","        epochs=1,\n","        batch_size=32,\n","        learning_rate=2e-5,\n","        margin=0.25\n","    )\n","    \n","    print(f\"Fine-tuning completed. Model saved to: {model_path}\")\n","    fine_tuned_model.half()\n","    return fine_tuned_model\n","\n","\n","def generate_rule_embeddings(test_df, model):\n","    \"\"\"Generate embeddings for each unique rule.\"\"\"\n","    print(\"Generating rule embeddings...\")\n","    \n","    unique_rules = test_df['rule'].unique()\n","    rule_embeddings = {}\n","    \n","    for rule in unique_rules:\n","        clean_rule = cleaner(str(rule))\n","        rule_emb = model.encode(\n","            clean_rule,\n","            convert_to_tensor=False,\n","            normalize_embeddings=True\n","        )\n","        rule_embeddings[rule] = rule_emb\n","        \n","    print(f\"Generated embeddings for {len(rule_embeddings)} rules\")\n","    return rule_embeddings\n","\n","\n","def create_rule_centroids(test_df, text_to_embedding, rule_embeddings):\n","    \"\"\"Create single centroid (mean) for positive and negative examples for each rule.\"\"\"\n","    print(f\"\\nCreating rule centroids (single mean centroid per type)...\")\n","\n","    rule_centroids = {}\n","\n","    for rule in test_df['rule'].unique():\n","        rule_data = test_df[test_df['rule'] == rule]\n","\n","        # Collect positive examples\n","        pos_embeddings = []\n","        for _, row in rule_data.iterrows():\n","            for col in ['positive_example_1', 'positive_example_2']:\n","                if pd.notna(row[col]):\n","                    clean_text = cleaner(str(row[col]))\n","                    if clean_text in text_to_embedding:\n","                        pos_embeddings.append(text_to_embedding[clean_text])\n","\n","        # Collect negative examples\n","        neg_embeddings = []\n","        for _, row in rule_data.iterrows():\n","            for col in ['negative_example_1', 'negative_example_2']:\n","                if pd.notna(row[col]):\n","                    clean_text = cleaner(str(row[col]))\n","                    if clean_text in text_to_embedding:\n","                        neg_embeddings.append(text_to_embedding[clean_text])\n","\n","        if pos_embeddings and neg_embeddings:\n","            pos_embeddings = np.array(pos_embeddings)\n","            neg_embeddings = np.array(neg_embeddings)\n","\n","            # Compute mean centroids\n","            pos_centroid = pos_embeddings.mean(axis=0)\n","            neg_centroid = neg_embeddings.mean(axis=0)\n","\n","            # Normalize centroids\n","            pos_centroid = pos_centroid / np.linalg.norm(pos_centroid)\n","            neg_centroid = neg_centroid / np.linalg.norm(neg_centroid)\n","\n","            rule_centroids[rule] = {\n","                'positive': pos_centroid,\n","                'negative': neg_centroid,\n","                'pos_count': len(pos_embeddings),\n","                'neg_count': len(neg_embeddings),\n","                'rule_embedding': rule_embeddings[rule]\n","            }\n","\n","            print(f\"  Rule: {rule[:50]}... - Pos: {len(pos_embeddings)}, Neg: {len(neg_embeddings)}\")\n","\n","    print(f\"Created centroids for {len(rule_centroids)} rules\")\n","    return rule_centroids\n","\n","\n","def predict_test_set(test_df, text_to_embedding, rule_centroids):\n","    \"\"\"Predict test set using Euclidean distance between body and pos/neg centroids.\"\"\"\n","    print(\"\\nMaking predictions on test set with Euclidean distance...\")\n","\n","    row_ids = []\n","    predictions = []\n","\n","    for rule in test_df['rule'].unique():\n","        print(f\"  Processing rule: {rule[:50]}...\")\n","        rule_data = test_df[test_df['rule'] == rule]\n","\n","        if rule not in rule_centroids:\n","            continue\n","\n","        pos_centroid = rule_centroids[rule]['positive']\n","        neg_centroid = rule_centroids[rule]['negative']\n","\n","        # Collect all valid embeddings and row_ids for this rule\n","        valid_embeddings = []\n","        valid_row_ids = []\n","\n","        for _, row in rule_data.iterrows():\n","            body = cleaner(str(row['body']))\n","            row_id = row['row_id']\n","\n","            if body in text_to_embedding:\n","                valid_embeddings.append(text_to_embedding[body])\n","                valid_row_ids.append(row_id)\n","\n","        if not valid_embeddings:\n","            continue\n","\n","        # Convert to numpy array\n","        query_embeddings = np.array(valid_embeddings)\n","\n","        # Compute Euclidean distances\n","        pos_distances = np.linalg.norm(query_embeddings - pos_centroid, axis=1)\n","        neg_distances = np.linalg.norm(query_embeddings - neg_centroid, axis=1)\n","\n","        # Score: closer to positive (lower distance) = higher violation score\n","        rule_predictions = neg_distances - pos_distances\n","\n","        row_ids.extend(valid_row_ids)\n","        predictions.extend(rule_predictions)\n","\n","    print(f\"Made predictions for {len(predictions)} test examples\")\n","    return row_ids, np.array(predictions)\n","\n","\n","\n","\n","def main():\n","    \"\"\"Main inference pipeline.\"\"\"\n","    print(\"=\"*70)\n","    print(\"SIMPLE SIMILARITY CLASSIFIER - INFERENCE\")\n","    print(\"=\"*70)\n","    \n","    # Step 1: Load test data\n","    test_df = load_test_data()\n","    \n","    # Step 2: Load or create fine-tuned model\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"MODEL PREPARATION PHASE\")\n","    print(\"=\"*50)\n","    model = load_or_create_finetuned_model(test_df)\n","    \n","    # Step 3: Collect all texts\n","    all_texts = collect_all_texts(test_df)\n","    \n","    # Step 4: Generate embeddings with fine-tuned model\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"EMBEDDING GENERATION PHASE\")\n","    print(\"=\"*50)\n","    all_embeddings = generate_embeddings(all_texts, model)\n","    \n","    # Step 5: Create text to embedding mapping\n","    text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n","    \n","    # Step 6: Generate rule embeddings\n","    rule_embeddings = generate_rule_embeddings(test_df, model)\n","    \n","    # Step 7: Create rule centroids from test examples\n","    rule_centroids = create_rule_centroids(test_df, text_to_embedding, rule_embeddings)\n","    \n","    # Step 8: Predict test set\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"PREDICTION PHASE\")\n","    print(\"=\"*50)\n","    row_ids, predictions = predict_test_set(test_df, text_to_embedding, rule_centroids)\n","\n","    min_vals = predictions.min(axis=0, keepdims=True)  #\n","    max_vals = predictions.max(axis=0, keepdims=True)  #\n","    range_vals = max_vals - min_vals\n","    range_vals = np.where(range_vals == 0, 1, range_vals)\n","    normalized = (predictions - min_vals) / range_vals\n","    \n","    # Step 9: Create submission with rule-conditioned scores\n","    submission_df = pd.DataFrame({\n","        'row_id': row_ids,\n","        'rule_violation': normalized\n","    })\n","    \n","    submission_df.to_csv('submission3.csv', index=False)\n","    print(f\"\\nSaved predictions for {len(submission_df)} test examples to submission.csv\")\n","    \n","    print(f\"\\n{'='*70}\")\n","    print(f\"FINE-TUNED EUCLIDEAN DISTANCE INFERENCE COMPLETED\")\n","    print(f\"Model: Fine-tuned BGE on test data triplets\")\n","    print(f\"Method: Single centroid with Euclidean distance\")\n","    print(f\"Predicted on {len(test_df)} test examples\")\n","    print(f\"Prediction stats: min={predictions.min():.4f}, max={predictions.max():.4f}, mean={predictions.mean():.4f}\")\n","    print(f\"{'='*70}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{},"source":["# Combined Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","q = pd.read_csv('submission1.csv')\n","l = pd.read_csv('submission5.csv')\n","m = pd.read_csv('submission3.csv')\n","\n","q = q.sort_values('row_id').reset_index(drop=True)\n","l = l.sort_values('row_id').reset_index(drop=True)\n","m = m.sort_values('row_id').reset_index(drop=True)\n","\n","assert (q['row_id'].values == l['row_id'].values).all(), \"row_id 不匹配：submission1 和 submission_qwen\"\n","assert (q['row_id'].values == m['row_id'].values).all(), \"row_id 不匹配：submission1 和 submission3\"\n","\n","def rank_normalize(series):\n","    return series.rank(method='average') / (len(series) + 1)\n","\n","rq = rank_normalize(q['rule_violation'])\n","rl = rank_normalize(l['rule_violation'])\n","rm = rank_normalize(m['rule_violation'])\n","\n","blend = 0.25 * rq + 0.25 * rm + 0.5 * rl\n","\n","q['rule_violation'] = blend\n","q.to_csv('/kaggle/working/submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":13121456,"isSourceIdPinned":false,"sourceId":94635,"sourceType":"competition"},{"sourceId":259545323,"sourceType":"kernelVersion"},{"sourceId":267553638,"sourceType":"kernelVersion"},{"sourceId":268327615,"sourceType":"kernelVersion"},{"isSourceIdPinned":false,"modelId":128845,"modelInstanceId":104636,"sourceId":124328,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":false,"modelId":128845,"modelInstanceId":104643,"sourceId":124335,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":31041,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
