{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":259545323,"sourceType":"kernelVersion"},{"sourceId":267553638,"sourceType":"kernelVersion"},{"sourceId":268327615,"sourceType":"kernelVersion"},{"sourceId":124328,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":104636,"modelId":128845},{"sourceId":124335,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":104643,"modelId":128845}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# model1 \n### 咸鱼号：Kaggle竞赛辅导","metadata":{}},{"cell_type":"code","source":"!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'deepspeed==0.17.4' -q\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T10:54:26.86261Z","iopub.execute_input":"2025-10-21T10:54:26.862853Z","iopub.status.idle":"2025-10-21T10:55:14.030493Z","shell.execute_reply.started":"2025-10-21T10:54:26.862801Z","shell.execute_reply":"2025-10-21T10:55:14.029761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile constants.py\n\nseed = 0\n\nbase_model_path = \"/kaggle/input/jigsaw-pretrain-public/pytorch/llama-3.2-3b-instruct/1\"\npretrain_lora_path = None\nlora_path = \"/kaggle/working/pseudo_lora\"\nuse_gptq = \"gptq\" in base_model_path\n\npositive = \"Yes\"\nnegative = \"No\"\njudge_words = \"Violation:\"\nsystem_prompt = '''You are given a comment from reddit and a rule. \nYour task is to classify whether the comment violates the rule. \nOnly respond Yes/No.'''\n\nfrac = 0.05\nuse_train = True\n\nimport kagglehub\n\ndeterministic = kagglehub.package_import('wasupandceacar/deterministic').deterministic\ndeterministic.init_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T10:58:39.67233Z","iopub.execute_input":"2025-10-21T10:58:39.673043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\n\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom constants import *\n\ndef build_prompt(row):\n    return f\"\"\"{system_prompt}\nSubreddit: r/{row[\"subreddit\"]}\nRule: {row[\"rule\"]}\nExamples:\n1) {row[\"positive_example\"]}\n{judge_words} Yes\n2) {row[\"negative_example\"]}\n{judge_words} No\nComment: {row[\"body\"]}\n{judge_words}\"\"\"\n\ndef get_df():\n    merge = list()\n    if use_train:\n        train_dataset = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\n        train_df = train_dataset[[\"body\", \"rule\", \"subreddit\", \"rule_violation\",\n                                \"positive_example_1\", \"positive_example_2\", \n                                \"negative_example_1\", \"negative_example_2\"]].copy()\n        train_df[\"positive_example\"] = np.where(np.random.rand(len(train_df)) < 0.5, train_df[\"positive_example_1\"], train_df[\"positive_example_2\"])\n        train_df[\"negative_example\"] = np.where(np.random.rand(len(train_df)) < 0.5, train_df[\"negative_example_1\"], train_df[\"negative_example_2\"])\n        train_df.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], inplace=True)\n        merge.append(train_df)\n    test_dataset = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n    test_dataset = test_dataset.groupby('rule', group_keys=False).apply(lambda x: x.sample(frac=frac, random_state=seed)).reset_index(drop=True)\n    print(f\"Select {len(test_dataset)} test data\")\n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 3):\n            sub_dataset = test_dataset[[\"rule\", \"subreddit\", \"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"]].copy()\n            body_col = f\"{violation_type}_example_{i}\"\n            other_positive_col = f\"{violation_type}_example_{3-i}\"\n            sub_dataset[\"body\"] = sub_dataset[body_col]\n            sub_dataset[f\"{violation_type}_example\"] = sub_dataset[other_positive_col]\n            anti_violation_type = \"negative\" if violation_type == \"positive\" else \"positive\"\n            sub_dataset[f\"{anti_violation_type}_example\"] = np.where(np.random.rand(len(sub_dataset)) < 0.5, sub_dataset[f\"{anti_violation_type}_example_1\"], sub_dataset[f\"{anti_violation_type}_example_2\"])\n            sub_dataset[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n            sub_dataset.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], inplace=True)\n            merge.append(sub_dataset)\n    return pd.concat(merge, axis=0).drop_duplicates(ignore_index=True)\n\ndef build_dataset(df):\n    df[\"prompt\"] = df.apply(build_prompt, axis=1)\n    columns = [\"prompt\"]\n    if \"rule_violation\" in df:\n        df[\"completion\"] = df[\"rule_violation\"].map({\n            1: positive,\n            0: negative,})\n        columns.append(\"completion\")\n    dataset = Dataset.from_pandas(df[columns])\n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\n\nimport torch\nimport pandas as pd\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import PeftModel, LoraConfig, get_peft_model\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom transformers.utils import is_torch_bf16_gpu_available\n\nfrom utils import *\nfrom constants import *\n\ndef main():\n    train_dataset = build_dataset(get_df())\n    lora_config = LoraConfig(\n        r=64,\n        lora_alpha=128,\n        lora_dropout=0.1,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n    )\n    \n    training_args = SFTConfig(\n        num_train_epochs=1,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_8bit\",\n        learning_rate=1e-4,\n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.03,\n        bf16=is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n        save_strategy=\"no\",\n        report_to=\"none\",\n        completion_only_loss=True,\n        packing=False,\n        remove_unused_columns=False,\n    )\n\n    if use_gptq:\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model_path,\n            device_map=\"balanced_low_0\",\n            trust_remote_code=True,\n            use_cache=False,\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            base_model_path,\n            quantization_config=BitsAndBytesConfig(\n                load_in_4bit=True,     \n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=True,\n            ),\n            device_map=\"balanced_low_0\",\n            trust_remote_code=True,\n            use_cache=False,\n        )\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n    tokenizer.pad_token = tokenizer.eos_token\n    if pretrain_lora_path:\n        model = PeftModel.from_pretrained(model, pretrain_lora_path)\n        model = model.merge_and_unload()\n\n    if len(train_dataset) > 0:\n        trainer = SFTTrainer(\n            model=model,\n            processing_class=tokenizer,\n            args=training_args,\n            train_dataset=train_dataset,\n            peft_config=lora_config,\n        )\n        trainer.train()\n        trainer.save_model(lora_path)\n    else:\n        peft_model = get_peft_model(model, lora_config)\n        peft_model.save_pretrained(lora_path)\n        tokenizer.save_pretrained(lora_path)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile accelerate_config.yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  gradient_clipping: 1.0\n  train_micro_batch_size_per_gpu: 4\n  \n  zero_stage: 2\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: false\n  \n  stage3_gather_16bit_weights_on_model_save: false\n  stage3_max_live_parameters: 1e8\n  stage3_max_reuse_distance: 1e8\n  stage3_prefetch_bucket_size: 5e7\n  stage3_param_persistence_threshold: 1e5\n  \n  zero_allow_untested_optimizer: true\n  zero_force_ds_cpu_optimizer: false\n  \n  # fp16:\n  #   enabled: true\n  #   loss_scale: 0\n  #   initial_scale_power: 16\n  #   loss_scale_window: 1000\n  #   hysteresis: 2\n  #   min_loss_scale: 1\n  bf16:\n    enabled: true\n  \ndistributed_type: DEEPSPEED\ndowncast_bf16: 'yes'\ndynamo_config:\n  dynamo_backend: INDUCTOR\n  dynamo_use_fullgraph: false\n  dynamo_use_dynamic: false\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile inference.py\n\nimport os\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nimport random\nimport vllm\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom vllm.lora.request import LoRARequest\nfrom utils import build_dataset\nfrom constants import *\nimport multiprocessing as mp\n\ndef run_inference_on_device(df_slice):\n    llm = vllm.LLM(\n        base_model_path,\n        quantization=\"gptq\" if use_gptq else None,\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2048,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=64,\n    )\n    tokenizer = llm.get_tokenizer()\n    outputs = llm.generate(\n        build_dataset(df_slice)[\"prompt\"],\n        vllm.SamplingParams(\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[MultipleChoiceLogitsProcessor(tokenizer, choices=[positive, negative])],\n            logprobs=2,\n        ),\n        use_tqdm=True,\n        lora_request=LoRARequest(\"lora1\", 1, lora_path)\n    )\n    log_probs = [{lp.decoded_token: np.exp(lp.logprob) for lp in out.outputs[0].logprobs[0].values()} for out in outputs]\n    predictions = pd.DataFrame(log_probs)[[positive, negative]]\n    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n    return predictions\n\ndef worker(device_id, df_slice, return_dict):\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n    preds = run_inference_on_device(df_slice)\n    return_dict[device_id] = preds\n\ndef main():\n    test_df = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n    test_df[\"positive_example\"] = test_df.apply(lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]), axis=1)\n    test_df[\"negative_example\"] = test_df.apply(lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]), axis=1)\n    test_df = test_df.drop(columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"], errors=\"ignore\")\n\n    mid = len(test_df) // 2\n    df0 = test_df.iloc[:mid].reset_index(drop=True)\n    df1 = test_df.iloc[mid:].reset_index(drop=True)\n\n    manager = mp.Manager()\n    return_dict = manager.dict()\n    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n    p0.start()\n    p1.start()\n    p0.join()\n    p1.join()\n\n    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n    submission = predictions[[\"row_id\", positive]].rename(columns={positive: \"rule_violation\"})\n    submission.to_csv(\"/kaggle/working/submission5.csv\", index=False)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --config_file accelerate_config.yaml train.py\n!python inference.py\n\nimport pandas as pd\npd.read_csv('/kaggle/working/submission5.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# model2 \n### 咸鱼号：Kaggle竞赛辅导","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom datasets import Dataset\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    models\n)\nfrom sentence_transformers.losses import TripletLoss\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom urllib.parse import urlparse\nimport faiss\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef cleaner(text):\n    \"\"\"Replace URLs with format: <url>: (domain/important-path)\"\"\"\n    if not text:\n        return text\n\n    # Regex pattern to match URLs\n    url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n\n    def replace_url(match):\n        url = match.group(0)\n        try:\n            parsed = urlparse(url)\n            domain = parsed.netloc.lower()\n            # Remove www. prefix if present\n            if domain.startswith('www.'):\n                domain = domain[4:]\n\n            # Extract meaningful path parts (first 1-2 segments)\n            path_parts = [part for part in parsed.path.split('/') if part]\n            if path_parts:\n                # Take first 1-2 meaningful path segments\n                important_path = '/'.join(path_parts[:2])\n                return f\"<url>: ({domain}/{important_path})\"\n            else:\n                return f\"<url>: ({domain})\"\n        except:\n            return \"<url>: (unknown)\"\n\n    return re.sub(url_pattern, replace_url, str(text))\n\n\ndef load_test_data():\n    \"\"\"Load test data.\"\"\"\n    print(\"Loading test data...\")\n    test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n    print(f\"Loaded {len(test_df)} test examples\")\n    print(f\"Unique rules: {test_df['rule'].nunique()}\")\n    return test_df\n\n\ndef collect_all_texts(test_df):\n    \"\"\"Collect all unique texts from test set.\"\"\"\n    print(\"\\nCollecting all texts for embedding...\")\n    \n    all_texts = set()\n    \n    # Add all bodies\n    for body in test_df['body']:\n        if pd.notna(body):\n            all_texts.add(cleaner(str(body)))\n    \n    # Add all positive and negative examples\n    example_cols = ['positive_example_1', 'positive_example_2', \n                   'negative_example_1', 'negative_example_2']\n    \n    for col in example_cols:\n        for example in test_df[col]:\n            if pd.notna(example):\n                all_texts.add(cleaner(str(example)))\n    \n    all_texts = list(all_texts)\n    print(f\"Collected {len(all_texts)} unique texts\")\n    return all_texts\n\n\ndef generate_embeddings(texts, model, batch_size=64):\n    \"\"\"Generate BGE embeddings for all texts.\"\"\"\n    print(f\"Generating embeddings for {len(texts)} texts...\")\n    \n    embeddings = model.encode(\n        sentences=texts,\n        batch_size=batch_size,\n        show_progress_bar=True,\n        convert_to_tensor=False,\n        normalize_embeddings=True\n    )\n    \n    return embeddings\n\n\ndef create_test_triplet_dataset(test_df, augmentation_factor=2, random_seed=42, subsample_fraction=1.0):\n    \"\"\"Create triplet dataset from test data: anchor=rule, positive=positive_example, negative=negative_example.\"\"\"\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    \n    anchors = []\n    positives = []\n    negatives = []\n    \n    print(\"Creating rule-aligned triplets from test data...\")\n    \n    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test rows\"):\n        rule = cleaner(str(row['rule']))\n        \n        pos_examples = []  # Will contain compliant comments (rule-aligned)\n        neg_examples = []  # Will contain violating comments (rule-misaligned)\n\n        for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n            if pd.notna(row[neg_col]):\n                pos_examples.append(cleaner(str(row[neg_col])))\n\n        for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n            if pd.notna(row[pos_col]):\n                neg_examples.append(cleaner(str(row[pos_col])))\n        \n        for pos_ex in pos_examples:\n            for neg_ex in neg_examples:\n                anchors.append(rule)\n                positives.append(pos_ex)\n                negatives.append(neg_ex)\n    \n    if augmentation_factor > 0:\n        print(f\"Adding {augmentation_factor}x augmentation...\")\n        \n        rule_positives = {}\n        rule_negatives = {}\n        \n        for rule in test_df['rule'].unique():\n            rule_df = test_df[test_df['rule'] == rule]\n            \n            pos_pool = []\n            neg_pool = []\n            \n            for _, row in rule_df.iterrows():\n                for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n                    if pd.notna(row[neg_col]):\n                        pos_pool.append(cleaner(str(row[neg_col])))\n                for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n                    if pd.notna(row[pos_col]):\n                        neg_pool.append(cleaner(str(row[pos_col])))\n            \n            rule_positives[rule] = list(set(pos_pool))\n            rule_negatives[rule] = list(set(neg_pool))\n        \n        for rule in test_df['rule'].unique():\n            clean_rule = cleaner(str(rule))\n            pos_pool = rule_positives[rule]\n            neg_pool = rule_negatives[rule]\n            \n            n_samples = min(augmentation_factor * len(pos_pool), len(pos_pool) * len(neg_pool))\n            \n            for _ in range(n_samples):\n                if pos_pool and neg_pool:\n                    anchors.append(clean_rule)\n                    positives.append(random.choice(pos_pool))\n                    negatives.append(random.choice(neg_pool))\n    \n    combined = list(zip(anchors, positives, negatives))\n    random.shuffle(combined)\n    \n    # Apply subsampling if requested\n    original_count = len(combined)\n    if subsample_fraction < 1.0:\n        n_samples = int(len(combined) * subsample_fraction)\n        combined = combined[:n_samples]\n        print(f\"Subsampled {original_count} -> {len(combined)} triplets ({subsample_fraction*100:.1f}%)\")\n    \n    anchors, positives, negatives = zip(*combined) if combined else ([], [], [])\n    \n    print(f\"Created {len(anchors)} triplets from test data\")\n    \n    dataset = Dataset.from_dict({\n        'anchor': list(anchors),\n        'positive': list(positives),\n        'negative': list(negatives)\n    })\n    \n    return dataset\n\n\ndef fine_tune_model(model, train_dataset, epochs=3, batch_size=32, learning_rate=2e-5, margin=0.25, output_dir=\"./models/test-finetuned-bge\"):\n    \"\"\"Fine-tune the sentence transformer model using triplet loss on test data.\"\"\"\n    \n    print(f\"Fine-tuning model on {len(train_dataset)} triplets...\")\n    \n    loss = TripletLoss(model=model, triplet_margin=margin)\n    \n    # Calculate max_steps for small datasets\n    dataset_size = len(train_dataset)\n    steps_per_epoch = max(1, dataset_size // batch_size)\n    max_steps = steps_per_epoch * epochs\n\n    args = SentenceTransformerTrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        warmup_steps=0,\n        learning_rate=learning_rate,\n        logging_steps=max(1, max_steps // 4),\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        fp16=True,\n        max_grad_norm=1.0,\n        dataloader_drop_last=False,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps = 1,\n        max_steps=max_steps,\n        report_to=\"none\"\n    )\n    \n    trainer = SentenceTransformerTrainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        loss=loss,\n    )\n    \n    trainer.train()\n    \n    final_model_path = f\"{output_dir}/final\"\n    print(f\"Saving fine-tuned model to {final_model_path}...\")\n    model.save_pretrained(final_model_path)\n    \n    return model, final_model_path\n\n\ndef load_or_create_finetuned_model(test_df):\n    \"\"\"Load fine-tuned model if exists, otherwise create and fine-tune it.\"\"\"\n    \n    fine_tuned_path = \"./models/test-finetuned-bge/final2\"\n    \n    if os.path.exists(fine_tuned_path):\n        print(f\"Loading existing fine-tuned model from {fine_tuned_path}...\")\n        try:\n            word_embedding_model = models.Transformer(fine_tuned_path, max_seq_length=128, do_lower_case=True)\n            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n            model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n            print(\"Loaded fine-tuned model with explicit pooling\")\n        except:\n            model = SentenceTransformer(fine_tuned_path)\n            print(\"Loaded fine-tuned model with default configuration\")\n        model.half()\n        return model\n    \n    print(\"Fine-tuned model not found. Creating new one...\")\n    \n    print(\"Loading base BGE embedding model...\")\n    # Try Kaggle path first, fallback to HuggingFace\n    try:\n        model_path = \"/kaggle/input/baai/transformers/bge-large-en-v1.5/1\"\n        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n        print(\"Loaded base model from Kaggle path with explicit pooling\")\n    except:\n        model_path = \"\"  # BAAI/bge-small-en-v1.5\n        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n        print(\"Loaded base model from local path with explicit pooling\")\n    \n    \n    triplet_dataset = create_test_triplet_dataset(test_df, augmentation_factor=2, subsample_fraction=1.)\n    \n    fine_tuned_model, model_path = fine_tune_model(\n        model=base_model,\n        train_dataset=triplet_dataset,\n        epochs=1,\n        batch_size=32,\n        learning_rate=2e-5,\n        margin=0.25\n    )\n    \n    print(f\"Fine-tuning completed. Model saved to: {model_path}\")\n    fine_tuned_model.half()\n    return fine_tuned_model\n\n\ndef generate_rule_embeddings(test_df, model):\n    \"\"\"Generate embeddings for each unique rule.\"\"\"\n    print(\"Generating rule embeddings...\")\n    \n    unique_rules = test_df['rule'].unique()\n    rule_embeddings = {}\n    \n    for rule in unique_rules:\n        clean_rule = cleaner(str(rule))\n        rule_emb = model.encode(\n            clean_rule,\n            convert_to_tensor=False,\n            normalize_embeddings=True\n        )\n        rule_embeddings[rule] = rule_emb\n        \n    print(f\"Generated embeddings for {len(rule_embeddings)} rules\")\n    return rule_embeddings\n\n\ndef create_rule_centroids(test_df, text_to_embedding, rule_embeddings):\n    \"\"\"Create single centroid (mean) for positive and negative examples for each rule.\"\"\"\n    print(f\"\\nCreating rule centroids (single mean centroid per type)...\")\n\n    rule_centroids = {}\n\n    for rule in test_df['rule'].unique():\n        rule_data = test_df[test_df['rule'] == rule]\n\n        # Collect positive examples\n        pos_embeddings = []\n        for _, row in rule_data.iterrows():\n            for col in ['positive_example_1', 'positive_example_2']:\n                if pd.notna(row[col]):\n                    clean_text = cleaner(str(row[col]))\n                    if clean_text in text_to_embedding:\n                        pos_embeddings.append(text_to_embedding[clean_text])\n\n        # Collect negative examples\n        neg_embeddings = []\n        for _, row in rule_data.iterrows():\n            for col in ['negative_example_1', 'negative_example_2']:\n                if pd.notna(row[col]):\n                    clean_text = cleaner(str(row[col]))\n                    if clean_text in text_to_embedding:\n                        neg_embeddings.append(text_to_embedding[clean_text])\n\n        if pos_embeddings and neg_embeddings:\n            pos_embeddings = np.array(pos_embeddings)\n            neg_embeddings = np.array(neg_embeddings)\n\n            # Compute mean centroids\n            pos_centroid = pos_embeddings.mean(axis=0)\n            neg_centroid = neg_embeddings.mean(axis=0)\n\n            # Normalize centroids\n            pos_centroid = pos_centroid / np.linalg.norm(pos_centroid)\n            neg_centroid = neg_centroid / np.linalg.norm(neg_centroid)\n\n            rule_centroids[rule] = {\n                'positive': pos_centroid,\n                'negative': neg_centroid,\n                'pos_count': len(pos_embeddings),\n                'neg_count': len(neg_embeddings),\n                'rule_embedding': rule_embeddings[rule]\n            }\n\n            print(f\"  Rule: {rule[:50]}... - Pos: {len(pos_embeddings)}, Neg: {len(neg_embeddings)}\")\n\n    print(f\"Created centroids for {len(rule_centroids)} rules\")\n    return rule_centroids\n\n\ndef predict_test_set(test_df, text_to_embedding, rule_centroids):\n    \"\"\"Predict test set using Euclidean distance between body and pos/neg centroids.\"\"\"\n    print(\"\\nMaking predictions on test set with Euclidean distance...\")\n\n    row_ids = []\n    predictions = []\n\n    for rule in test_df['rule'].unique():\n        print(f\"  Processing rule: {rule[:50]}...\")\n        rule_data = test_df[test_df['rule'] == rule]\n\n        if rule not in rule_centroids:\n            continue\n\n        pos_centroid = rule_centroids[rule]['positive']\n        neg_centroid = rule_centroids[rule]['negative']\n\n        # Collect all valid embeddings and row_ids for this rule\n        valid_embeddings = []\n        valid_row_ids = []\n\n        for _, row in rule_data.iterrows():\n            body = cleaner(str(row['body']))\n            row_id = row['row_id']\n\n            if body in text_to_embedding:\n                valid_embeddings.append(text_to_embedding[body])\n                valid_row_ids.append(row_id)\n\n        if not valid_embeddings:\n            continue\n\n        # Convert to numpy array\n        query_embeddings = np.array(valid_embeddings)\n\n        # Compute Euclidean distances\n        pos_distances = np.linalg.norm(query_embeddings - pos_centroid, axis=1)\n        neg_distances = np.linalg.norm(query_embeddings - neg_centroid, axis=1)\n\n        # Score: closer to positive (lower distance) = higher violation score\n        rule_predictions = neg_distances - pos_distances\n\n        row_ids.extend(valid_row_ids)\n        predictions.extend(rule_predictions)\n\n    print(f\"Made predictions for {len(predictions)} test examples\")\n    return row_ids, np.array(predictions)\n\n\n\n\ndef main():\n    \"\"\"Main inference pipeline.\"\"\"\n    print(\"=\"*70)\n    print(\"SIMPLE SIMILARITY CLASSIFIER - INFERENCE\")\n    print(\"=\"*70)\n    \n    # Step 1: Load test data\n    test_df = load_test_data()\n    \n    # Step 2: Load or create fine-tuned model\n    print(\"\\n\" + \"=\"*50)\n    print(\"MODEL PREPARATION PHASE\")\n    print(\"=\"*50)\n    model = load_or_create_finetuned_model(test_df)\n    \n    # Step 3: Collect all texts\n    all_texts = collect_all_texts(test_df)\n    \n    # Step 4: Generate embeddings with fine-tuned model\n    print(\"\\n\" + \"=\"*50)\n    print(\"EMBEDDING GENERATION PHASE\")\n    print(\"=\"*50)\n    all_embeddings = generate_embeddings(all_texts, model)\n    \n    # Step 5: Create text to embedding mapping\n    text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n    \n    # Step 6: Generate rule embeddings\n    rule_embeddings = generate_rule_embeddings(test_df, model)\n    \n    # Step 7: Create rule centroids from test examples\n    rule_centroids = create_rule_centroids(test_df, text_to_embedding, rule_embeddings)\n    \n    # Step 8: Predict test set\n    print(\"\\n\" + \"=\"*50)\n    print(\"PREDICTION PHASE\")\n    print(\"=\"*50)\n    row_ids, predictions = predict_test_set(test_df, text_to_embedding, rule_centroids)\n    \n    min_vals = predictions.min(axis=0, keepdims=True)  #\n    max_vals = predictions.max(axis=0, keepdims=True)  #\n    range_vals = max_vals - min_vals\n    range_vals = np.where(range_vals == 0, 1, range_vals)\n    normalized = (predictions - min_vals) / range_vals\n    \n    # Step 9: Create submission with rule-conditioned scores\n    submission_df = pd.DataFrame({\n        'row_id': row_ids,\n        'rule_violation': normalized\n    })\n    \n    \n    submission_df.to_csv('submission1.csv', index=False)\n    print(f\"\\nSaved predictions for {len(submission_df)} test examples to submission1.csv\")\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"FINE-TUNED EUCLIDEAN DISTANCE INFERENCE COMPLETED\")\n    print(f\"Model: Fine-tuned BGE on test data triplets\")\n    print(f\"Method: Single centroid with Euclidean distance\")\n    print(f\"Predicted on {len(test_df)} test examples\")\n    print(f\"Prediction stats: min={predictions.min():.4f}, max={predictions.max():.4f}, mean={predictions.mean():.4f}\")\n    print(f\"{'='*70}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-21T10:55:30.009993Z","iopub.status.idle":"2025-10-21T10:55:30.010213Z","shell.execute_reply.started":"2025-10-21T10:55:30.010101Z","shell.execute_reply":"2025-10-21T10:55:30.01011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# model3 \n### 咸鱼号：Kaggle竞赛辅导","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom datasets import Dataset\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    models\n)\nfrom sentence_transformers.losses import TripletLoss\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nfrom urllib.parse import urlparse\nimport faiss\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef cleaner(text):\n    \"\"\"Replace URLs with format: <url>: (domain/important-path)\"\"\"\n    if not text:\n        return text\n\n    # Regex pattern to match URLs\n    url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n\n    def replace_url(match):\n        url = match.group(0)\n        try:\n            parsed = urlparse(url)\n            domain = parsed.netloc.lower()\n            # Remove www. prefix if present\n            if domain.startswith('www.'):\n                domain = domain[4:]\n\n            # Extract meaningful path parts (first 1-2 segments)\n            path_parts = [part for part in parsed.path.split('/') if part]\n            if path_parts:\n                # Take first 1-2 meaningful path segments\n                important_path = '/'.join(path_parts[:2])\n                return f\"<url>: ({domain}/{important_path})\"\n            else:\n                return f\"<url>: ({domain})\"\n        except:\n            return \"<url>: (unknown)\"\n\n    return re.sub(url_pattern, replace_url, str(text))\n\n\ndef load_test_data():\n    \"\"\"Load test data.\"\"\"\n    print(\"Loading test data...\")\n    test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n    print(f\"Loaded {len(test_df)} test examples\")\n    print(f\"Unique rules: {test_df['rule'].nunique()}\")\n    return test_df\n\n\ndef collect_all_texts(test_df):\n    \"\"\"Collect all unique texts from test set.\"\"\"\n    print(\"\\nCollecting all texts for embedding...\")\n    \n    all_texts = set()\n    \n    # Add all bodies\n    for body in test_df['body']:\n        if pd.notna(body):\n            all_texts.add(cleaner(str(body)))\n    \n    # Add all positive and negative examples\n    example_cols = ['positive_example_1', 'positive_example_2', \n                   'negative_example_1', 'negative_example_2']\n    \n    for col in example_cols:\n        for example in test_df[col]:\n            if pd.notna(example):\n                all_texts.add(cleaner(str(example)))\n    \n    all_texts = list(all_texts)\n    print(f\"Collected {len(all_texts)} unique texts\")\n    return all_texts\n\n\ndef generate_embeddings(texts, model, batch_size=64):\n    \"\"\"Generate BGE embeddings for all texts.\"\"\"\n    print(f\"Generating embeddings for {len(texts)} texts...\")\n    \n    embeddings = model.encode(\n        sentences=texts,\n        batch_size=batch_size,\n        show_progress_bar=True,\n        convert_to_tensor=False,\n        normalize_embeddings=True\n    )\n    \n    return embeddings\n\n\ndef create_test_triplet_dataset(test_df, augmentation_factor=2, random_seed=42, subsample_fraction=1.0):\n    \"\"\"Create triplet dataset from test data: anchor=rule, positive=positive_example, negative=negative_example.\"\"\"\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    \n    anchors = []\n    positives = []\n    negatives = []\n    \n    print(\"Creating rule-aligned triplets from test data...\")\n    \n    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing test rows\"):\n        rule = cleaner(str(row['rule']))\n        \n        pos_examples = []  # Will contain compliant comments (rule-aligned)\n        neg_examples = []  # Will contain violating comments (rule-misaligned)\n\n        for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n            if pd.notna(row[neg_col]):\n                pos_examples.append(cleaner(str(row[neg_col])))\n\n        for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n            if pd.notna(row[pos_col]):\n                neg_examples.append(cleaner(str(row[pos_col])))\n        \n        for pos_ex in pos_examples:\n            for neg_ex in neg_examples:\n                anchors.append(rule)\n                positives.append(pos_ex)\n                negatives.append(neg_ex)\n    \n    if augmentation_factor > 0:\n        print(f\"Adding {augmentation_factor}x augmentation...\")\n        \n        rule_positives = {}\n        rule_negatives = {}\n        \n        for rule in test_df['rule'].unique():\n            rule_df = test_df[test_df['rule'] == rule]\n            \n            pos_pool = []\n            neg_pool = []\n            \n            for _, row in rule_df.iterrows():\n                for neg_col in ['negative_example_1', 'negative_example_2']:  # Compliant → triplet positive\n                    if pd.notna(row[neg_col]):\n                        pos_pool.append(cleaner(str(row[neg_col])))\n                for pos_col in ['positive_example_1', 'positive_example_2']:  # Violating → triplet negative\n                    if pd.notna(row[pos_col]):\n                        neg_pool.append(cleaner(str(row[pos_col])))\n            \n            rule_positives[rule] = list(set(pos_pool))\n            rule_negatives[rule] = list(set(neg_pool))\n        \n        for rule in test_df['rule'].unique():\n            clean_rule = cleaner(str(rule))\n            pos_pool = rule_positives[rule]\n            neg_pool = rule_negatives[rule]\n            \n            n_samples = min(augmentation_factor * len(pos_pool), len(pos_pool) * len(neg_pool))\n            \n            for _ in range(n_samples):\n                if pos_pool and neg_pool:\n                    anchors.append(clean_rule)\n                    positives.append(random.choice(pos_pool))\n                    negatives.append(random.choice(neg_pool))\n    \n    combined = list(zip(anchors, positives, negatives))\n    random.shuffle(combined)\n    \n    # Apply subsampling if requested\n    original_count = len(combined)\n    if subsample_fraction < 1.0:\n        n_samples = int(len(combined) * subsample_fraction)\n        combined = combined[:n_samples]\n        print(f\"Subsampled {original_count} -> {len(combined)} triplets ({subsample_fraction*100:.1f}%)\")\n    \n    anchors, positives, negatives = zip(*combined) if combined else ([], [], [])\n    \n    print(f\"Created {len(anchors)} triplets from test data\")\n    \n    dataset = Dataset.from_dict({\n        'anchor': list(anchors),\n        'positive': list(positives),\n        'negative': list(negatives)\n    })\n    \n    return dataset\n\n\ndef fine_tune_model(model, train_dataset, epochs=3, batch_size=32, learning_rate=2e-5, margin=0.25, output_dir=\"./models/test-finetuned-bge\"):\n    \"\"\"Fine-tune the sentence transformer model using triplet loss on test data.\"\"\"\n    \n    print(f\"Fine-tuning model on {len(train_dataset)} triplets...\")\n    \n    loss = TripletLoss(model=model, triplet_margin=margin)\n    \n    # Calculate max_steps for small datasets\n    dataset_size = len(train_dataset)\n    steps_per_epoch = max(1, dataset_size // batch_size)\n    max_steps = steps_per_epoch * epochs\n\n    args = SentenceTransformerTrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        warmup_steps=0,\n        learning_rate=learning_rate,\n        logging_steps=max(1, max_steps // 4),\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        fp16=True,\n        max_grad_norm=1.0,\n        dataloader_drop_last=False,\n        gradient_checkpointing=True,\n        gradient_accumulation_steps = 1,\n        max_steps=max_steps,\n        report_to=\"none\"\n    )\n    \n    trainer = SentenceTransformerTrainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        loss=loss,\n    )\n    \n    trainer.train()\n    \n    final_model_path = f\"{output_dir}/final\"\n    print(f\"Saving fine-tuned model to {final_model_path}...\")\n    model.save_pretrained(final_model_path)\n    \n    return model, final_model_path\n\n\ndef load_or_create_finetuned_model(test_df):\n    \"\"\"Load fine-tuned model if exists, otherwise create and fine-tune it.\"\"\"\n    \n    fine_tuned_path = \"./models/test-finetuned-bge/final3\"\n    \n    if os.path.exists(fine_tuned_path):\n        print(f\"Loading existing fine-tuned model from {fine_tuned_path}...\")\n        try:\n            word_embedding_model = models.Transformer(fine_tuned_path, max_seq_length=128, do_lower_case=True)\n            pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n            model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n            print(\"Loaded fine-tuned model with explicit pooling\")\n        except:\n            model = SentenceTransformer(fine_tuned_path)\n            print(\"Loaded fine-tuned model with default configuration\")\n        model.half()\n        return model\n    \n    print(\"Fine-tuned model not found. Creating new one...\")\n    \n    print(\"Loading base BGE embedding model...\")\n    # Try Kaggle path first, fallback to HuggingFace\n    try:\n        model_path = \"/kaggle/input/baai/transformers/bge-base-en-v1.5/1\"\n        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n        print(\"Loaded base model from Kaggle path with explicit pooling\")\n    except:\n        model_path = \"\"  # BAAI/bge-small-en-v1.5\n        word_embedding_model = models.Transformer(model_path, max_seq_length=128, do_lower_case=True)\n        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\")\n        base_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n        print(\"Loaded base model from local path with explicit pooling\")\n    \n    \n    triplet_dataset = create_test_triplet_dataset(test_df, augmentation_factor=2, subsample_fraction=1.)\n    \n    fine_tuned_model, model_path = fine_tune_model(\n        model=base_model,\n        train_dataset=triplet_dataset,\n        epochs=1,\n        batch_size=32,\n        learning_rate=2e-5,\n        margin=0.25\n    )\n    \n    print(f\"Fine-tuning completed. Model saved to: {model_path}\")\n    fine_tuned_model.half()\n    return fine_tuned_model\n\n\ndef generate_rule_embeddings(test_df, model):\n    \"\"\"Generate embeddings for each unique rule.\"\"\"\n    print(\"Generating rule embeddings...\")\n    \n    unique_rules = test_df['rule'].unique()\n    rule_embeddings = {}\n    \n    for rule in unique_rules:\n        clean_rule = cleaner(str(rule))\n        rule_emb = model.encode(\n            clean_rule,\n            convert_to_tensor=False,\n            normalize_embeddings=True\n        )\n        rule_embeddings[rule] = rule_emb\n        \n    print(f\"Generated embeddings for {len(rule_embeddings)} rules\")\n    return rule_embeddings\n\n\ndef create_rule_centroids(test_df, text_to_embedding, rule_embeddings):\n    \"\"\"Create single centroid (mean) for positive and negative examples for each rule.\"\"\"\n    print(f\"\\nCreating rule centroids (single mean centroid per type)...\")\n\n    rule_centroids = {}\n\n    for rule in test_df['rule'].unique():\n        rule_data = test_df[test_df['rule'] == rule]\n\n        # Collect positive examples\n        pos_embeddings = []\n        for _, row in rule_data.iterrows():\n            for col in ['positive_example_1', 'positive_example_2']:\n                if pd.notna(row[col]):\n                    clean_text = cleaner(str(row[col]))\n                    if clean_text in text_to_embedding:\n                        pos_embeddings.append(text_to_embedding[clean_text])\n\n        # Collect negative examples\n        neg_embeddings = []\n        for _, row in rule_data.iterrows():\n            for col in ['negative_example_1', 'negative_example_2']:\n                if pd.notna(row[col]):\n                    clean_text = cleaner(str(row[col]))\n                    if clean_text in text_to_embedding:\n                        neg_embeddings.append(text_to_embedding[clean_text])\n\n        if pos_embeddings and neg_embeddings:\n            pos_embeddings = np.array(pos_embeddings)\n            neg_embeddings = np.array(neg_embeddings)\n\n            # Compute mean centroids\n            pos_centroid = pos_embeddings.mean(axis=0)\n            neg_centroid = neg_embeddings.mean(axis=0)\n\n            # Normalize centroids\n            pos_centroid = pos_centroid / np.linalg.norm(pos_centroid)\n            neg_centroid = neg_centroid / np.linalg.norm(neg_centroid)\n\n            rule_centroids[rule] = {\n                'positive': pos_centroid,\n                'negative': neg_centroid,\n                'pos_count': len(pos_embeddings),\n                'neg_count': len(neg_embeddings),\n                'rule_embedding': rule_embeddings[rule]\n            }\n\n            print(f\"  Rule: {rule[:50]}... - Pos: {len(pos_embeddings)}, Neg: {len(neg_embeddings)}\")\n\n    print(f\"Created centroids for {len(rule_centroids)} rules\")\n    return rule_centroids\n\n\ndef predict_test_set(test_df, text_to_embedding, rule_centroids):\n    \"\"\"Predict test set using Euclidean distance between body and pos/neg centroids.\"\"\"\n    print(\"\\nMaking predictions on test set with Euclidean distance...\")\n\n    row_ids = []\n    predictions = []\n\n    for rule in test_df['rule'].unique():\n        print(f\"  Processing rule: {rule[:50]}...\")\n        rule_data = test_df[test_df['rule'] == rule]\n\n        if rule not in rule_centroids:\n            continue\n\n        pos_centroid = rule_centroids[rule]['positive']\n        neg_centroid = rule_centroids[rule]['negative']\n\n        # Collect all valid embeddings and row_ids for this rule\n        valid_embeddings = []\n        valid_row_ids = []\n\n        for _, row in rule_data.iterrows():\n            body = cleaner(str(row['body']))\n            row_id = row['row_id']\n\n            if body in text_to_embedding:\n                valid_embeddings.append(text_to_embedding[body])\n                valid_row_ids.append(row_id)\n\n        if not valid_embeddings:\n            continue\n\n        # Convert to numpy array\n        query_embeddings = np.array(valid_embeddings)\n\n        # Compute Euclidean distances\n        pos_distances = np.linalg.norm(query_embeddings - pos_centroid, axis=1)\n        neg_distances = np.linalg.norm(query_embeddings - neg_centroid, axis=1)\n\n        # Score: closer to positive (lower distance) = higher violation score\n        rule_predictions = neg_distances - pos_distances\n\n        row_ids.extend(valid_row_ids)\n        predictions.extend(rule_predictions)\n\n    print(f\"Made predictions for {len(predictions)} test examples\")\n    return row_ids, np.array(predictions)\n\n\n\n\ndef main():\n    \"\"\"Main inference pipeline.\"\"\"\n    print(\"=\"*70)\n    print(\"SIMPLE SIMILARITY CLASSIFIER - INFERENCE\")\n    print(\"=\"*70)\n    \n    # Step 1: Load test data\n    test_df = load_test_data()\n    \n    # Step 2: Load or create fine-tuned model\n    print(\"\\n\" + \"=\"*50)\n    print(\"MODEL PREPARATION PHASE\")\n    print(\"=\"*50)\n    model = load_or_create_finetuned_model(test_df)\n    \n    # Step 3: Collect all texts\n    all_texts = collect_all_texts(test_df)\n    \n    # Step 4: Generate embeddings with fine-tuned model\n    print(\"\\n\" + \"=\"*50)\n    print(\"EMBEDDING GENERATION PHASE\")\n    print(\"=\"*50)\n    all_embeddings = generate_embeddings(all_texts, model)\n    \n    # Step 5: Create text to embedding mapping\n    text_to_embedding = {text: emb for text, emb in zip(all_texts, all_embeddings)}\n    \n    # Step 6: Generate rule embeddings\n    rule_embeddings = generate_rule_embeddings(test_df, model)\n    \n    # Step 7: Create rule centroids from test examples\n    rule_centroids = create_rule_centroids(test_df, text_to_embedding, rule_embeddings)\n    \n    # Step 8: Predict test set\n    print(\"\\n\" + \"=\"*50)\n    print(\"PREDICTION PHASE\")\n    print(\"=\"*50)\n    row_ids, predictions = predict_test_set(test_df, text_to_embedding, rule_centroids)\n\n    min_vals = predictions.min(axis=0, keepdims=True)  #\n    max_vals = predictions.max(axis=0, keepdims=True)  #\n    range_vals = max_vals - min_vals\n    range_vals = np.where(range_vals == 0, 1, range_vals)\n    normalized = (predictions - min_vals) / range_vals\n    \n    # Step 9: Create submission with rule-conditioned scores\n    submission_df = pd.DataFrame({\n        'row_id': row_ids,\n        'rule_violation': normalized\n    })\n    \n    submission_df.to_csv('submission3.csv', index=False)\n    print(f\"\\nSaved predictions for {len(submission_df)} test examples to submission.csv\")\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"FINE-TUNED EUCLIDEAN DISTANCE INFERENCE COMPLETED\")\n    print(f\"Model: Fine-tuned BGE on test data triplets\")\n    print(f\"Method: Single centroid with Euclidean distance\")\n    print(f\"Predicted on {len(test_df)} test examples\")\n    print(f\"Prediction stats: min={predictions.min():.4f}, max={predictions.max():.4f}, mean={predictions.mean():.4f}\")\n    print(f\"{'='*70}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 融合提交\n### 咸鱼号：Kaggle竞赛辅导","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nq = pd.read_csv('submission1.csv')\nl = pd.read_csv('submission5.csv')\nm = pd.read_csv('submission3.csv')\n\nq = q.sort_values('row_id').reset_index(drop=True)\nl = l.sort_values('row_id').reset_index(drop=True)\nm = m.sort_values('row_id').reset_index(drop=True)\n\nassert (q['row_id'].values == l['row_id'].values).all(), \"row_id 不匹配：submission1 和 submission_qwen\"\nassert (q['row_id'].values == m['row_id'].values).all(), \"row_id 不匹配：submission1 和 submission3\"\n\ndef rank_normalize(series):\n    return series.rank(method='average') / (len(series) + 1)\n\nrq = rank_normalize(q['rule_violation'])\nrl = rank_normalize(l['rule_violation'])\nrm = rank_normalize(m['rule_violation'])\n\nblend = 0.25 * rq + 0.25 * rm + 0.5 * rl\n\nq['rule_violation'] = blend\nq.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}